[{"authors":["admin"],"categories":null,"content":"My name is Jason Zivkovic and I’m a Data Scientist at the Reece Group.\nGrowing up as a sports-mad child and teen, statistics were always in my world. But with the love of sport blinding me, I didn’t even realise it was a love of statistics and data. Then, after meandering (much to my mother’s chagrin) through my early work life, and dabbling with different careers, I discovered edX and DataCamp and the programming-focused data analysis short courses they had available. I started taking them and my love of the data was rekindled! Life was made a lot easier in that I had two managers in a row who’s thirst for analytics was as prominent as mine and they gave me the freedom to learn, explore and discover endless possibilities. To them I will be forever greatful.\nKaggle was where I first started dabbling in small analyses - the datasets are as diverse as any wide-eyed newbie could wish for. I still have a look from time to time and think I should write another kernel. I got some great help from the Kaggle community and the code bases there are really good for any new starter. You can find my profile here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"My name is Jason Zivkovic and I’m a Data Scientist at the Reece Group.\nGrowing up as a sports-mad child and teen, statistics were always in my world. But with the love of sport blinding me, I didn’t even realise it was a love of statistics and data.","tags":null,"title":"Jason Zivkovic","type":"authors"},{"authors":null,"categories":["R","worldfootballR"],"content":" With the creation of the worldfootballR R package (a new R package to aid in the extraction of Football (Soccer) data from fbref), I will be trying to highlight ways the package can be used.\nIn this piece, I will analyse Liverpool’s use of substitutions in the English Premier League (EPL) over the last three seasons to understand whether there has been a change in behaviour in the currrent season, and whether there was a change around the COVID outbreak and subsequent pause taken during the 2019-2020 season.\nData was extracted for the last two and a half seasons, up to the end of matchweek 17 in the 2020/21 EPL season.\nExtract Data Using worldfootballR The package has a function called get_match_summary which extracts match summary (goals, subs, red/yellow cards) data for a match URLs.\nTo get the match URLs, another function - get_match_urls - can be used, which accepts a country code (“ENG” for England), gender and the year the season(s) ended, and returns a vector of match URLs.\ndevtools::install_github(\u0026quot;JaseZiv/worldfootballR\u0026quot;) library(worldfootballR) match_urls \u0026lt;- get_match_urls(country = \u0026quot;ENG\u0026quot;, gender = \u0026quot;M\u0026quot;, season_end_year = c(2019:2021)) match_summaries \u0026lt;- get_match_summary(match_url = match_urls)  Teams making their first sub Now that the package instructions are out of the way, we turn our attention to the analysis of substitution data in the Premier League, and specifically ask the question, has there been a change in Liverpool’s substitution behaviour.\nThe analysis was loosely inspired by a piece done by FiveThirtyEight on Bundesliga team susbstitutions before and after the league restart.\nAll Teams When the COVID-19 pandemic swept across the nation, some of the major leagues were forced to pause their seasons while cases became manageable.\nDuring the 2019-20 season before the virus halted play, teams were typically making their first sub at the 56th minute, which was consistent with that of the 2018-19 season, however once play resumed (and teams were allowed to make more subs), that number dropped to 46 minutes (half-time typically).\nThis current season has seen the teams’ behaviour revert back to the pre-restarted season numbers (54 minutes).\n What about Liverpool; where do they sit? In all seasons analysed, Liverpool make slightly more subs in a season compared to the league average, with the average number of subs in a season peaking after the restart last season to 4.67 subs per game. The current season has seen this revert to pre-COVID levels.\nInterestingly, while Liverpool’s first sub typically comes slightly later than the league average (56 minutes compared to 52 minutes), the Reds have stuck close to their first sub time after the season restart, while the league average has reverted back to pre-COVID levels, reversing the league-wide trend.\n And the first hour? The Reds made 12% of their substitutions in the first hour during the combined 2019-20 season (double that of their 2018-19 season), and this season has seen this percentage again double through the first 16 games. The league however has experienced a direct reversal of this trend, spiking to 24% during the restarted part of the 19/20 season, and settling back down at the league average for the previous seasons.\nIt will be interesting to see how this trend continues throughout the latter parts of this season.\n@import url(\"https://fonts.googleapis.com/css2?family=Chivo:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900\u0026display=swap\"); html { font-family: Chivo, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #kbcypgyzgc .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 3px; border-top-color: transparent; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 3px; border-bottom-color: transparent; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #kbcypgyzgc .gt_heading { background-color: #FFFFFF; text-align: left; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #kbcypgyzgc .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #kbcypgyzgc .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #kbcypgyzgc .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kbcypgyzgc .gt_col_headings { border-top-style: solid; border-top-width: 3px; border-top-color: transparent; border-bottom-style: solid; border-bottom-width: 3px; border-bottom-color: black; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #kbcypgyzgc .gt_col_heading { color: #333333; background-color: white; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #kbcypgyzgc .gt_column_spanner_outer { color: #333333; background-color: white; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #kbcypgyzgc .gt_column_spanner_outer:first-child { padding-left: 0; } #kbcypgyzgc .gt_column_spanner_outer:last-child { padding-right: 0; } #kbcypgyzgc .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 3px; border-bottom-color: black; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #kbcypgyzgc .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #kbcypgyzgc .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #kbcypgyzgc .gt_from_md  :first-child { margin-top: 0; } #kbcypgyzgc .gt_from_md  :last-child { margin-bottom: 0; } #kbcypgyzgc .gt_row { padding-top: 3px; padding-bottom: 3px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #kbcypgyzgc .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #kbcypgyzgc .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #kbcypgyzgc .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #kbcypgyzgc .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #kbcypgyzgc .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #kbcypgyzgc .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #kbcypgyzgc .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kbcypgyzgc .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #kbcypgyzgc .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #kbcypgyzgc .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #kbcypgyzgc .gt_sourcenote { font-size: 12px; padding: 4px; } #kbcypgyzgc .gt_left { text-align: left; } #kbcypgyzgc .gt_center { text-align: center; } #kbcypgyzgc .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #kbcypgyzgc .gt_font_normal { font-weight: normal; } #kbcypgyzgc .gt_font_bold { font-weight: bold; } #kbcypgyzgc .gt_font_italic { font-style: italic; } #kbcypgyzgc .gt_super { font-size: 65%; } #kbcypgyzgc .gt_footnote_marks { font-style: italic; font-size: 65%; }   LIVERPOOL SUBBING MORE IN THE FIRST 60 MINUTES   The rate that Liverpool are subbing in the first 60 minutes has doubled from last season, with 1 in 4 subs coming in this time period - double last season's rate     Number of Subs  Avg Time to 1st Sub  Subs/Game  Share Subs in 1st 60    Liverpool Rest of League1 Liverpool Rest of League1 Liverpool Rest of League1 Liverpool Rest of League1    2018-2019 112 108 64 51 2.95 2.85 6% 18%   2019-2020 - Before Restart2 87 79 64 50 3.00 2.77 12% 18%   2019-2020 - After Restart2 42 35 57 49 4.67 3.77 12% 24%   2020-2021 47 45 56 52 2.94 2.74 24% 18%    SOURCE: worldfootballR\nTABLE: @jase_ziv    1  All clubs excluding Liverpool 2  COVID-interrupted Season       Wrap Up This was the first in a series of analyses that will make use of various data extraction functions in the worldfootballR package.\nHere we saw that Liverpool are making their first substitution earlier than their previous trends, and are deviating from the league averages. I will follow this analysis to see if this trend has continued.\nAs always, any questions/comments about the piece or the R package, feel free to reach out through the regular channels.\nGO REDS!!\n ","date":1610236800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610236800,"objectID":"9a36463a94f88929c3d82d507cf1f9db","permalink":"/blog/liverpools-early-substitutions-worldfootballr/","publishdate":"2021-01-10T00:00:00Z","relpermalink":"/blog/liverpools-early-substitutions-worldfootballr/","section":"post","summary":"With the creation of the worldfootballR R package (a new R package to aid in the extraction of Football (Soccer) data from fbref), I will be trying to highlight ways the package can be used.","tags":["worldfootballR","R","soccer","soccer analytics","ggplot2","gt"],"title":"Liverpool's Earlier Subsititutions - an Introduction to worldfootballR","type":"post"},{"authors":null,"categories":null,"content":"The worldfootballR package is designed to allow users to extract world football (soccer) data from the very popular and thorough website fbref.com.\nThe vignette for the package offers a full description of the package and the functions available, and can be found here.\n","date":1610197200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610197200,"objectID":"627a30a73197b9141023b017683b267c","permalink":"/project/worldfootballr/","publishdate":"2021-01-10T00:00:00+11:00","relpermalink":"/project/worldfootballr/","section":"project","summary":"An R package developed to aid in the extraction of world football (soccer) data from fbref.com","tags":["R Package","worldfootballR"],"title":"worldfootballR R Package","type":"project"},{"authors":null,"categories":null,"content":"The chessR package is designed to allow users to extract their game data and other data from popular online chess platforms, including chess.com and Lichess. The websites offer a very convenient set of APIs to be able to access date through.\nThe vignette for the package offers a full description of the package and the functions available, and can be found here.\n","date":1587996000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587996000,"objectID":"10eada3ca2013f82df6fdc6c379c2f7a","permalink":"/project/chessr/","publishdate":"2020-04-28T00:00:00+10:00","relpermalink":"/project/chessr/","section":"project","summary":"An R package developed to aid in the extraction and analysis of data from popular online chess platforms","tags":["R Package"],"title":"chessR R Package","type":"project"},{"authors":null,"categories":["R","sports analytics","pythagorean expectation"],"content":" Introduction This piece is the first part in a series of posts I will be releasing that will look to analyse how many wins teams should’ve won given their performances over the season and compare them to their actual wins achieved. To achieve this goal, it will apply a commonly known method in the sports analytics community called Pythagorean Expectation.\nSports in the Report: The series of posts will look to analyse a number of very popular sports leagues around the globe (ie the NBA), and in some sports’ cases, the analysis will span multiple leagues (ie professional soccer has a number of “major” leagues around the globe).\nThe following professional sports/sporting leagues will be included in the series:\n College Basketball (Men’s and Women’s) NBA NFL NHL Soccer (Football) Australian Rules Football (AFL)  As they become available, you’ll be able to click the link to them.\n  Pythagorean Expectation GM of the Houston Rockets Daryl Morey. Source: Northwestern.edu\n While we are effortlessly able to count the wins and losses a team experiences, we might want to be able to assess the number of wins they had, versus how many wins they should have had, given their performances during the season. This might be able to tell us something about whether any luck or other factors played a part in a team’s success (or failure).\nOk great, so how do we know how many wins our team was supposed to get?\nEnter, Pythagorean Expectation. While many question whether he was actually the creator of the Pythagorean Theorem, the famed philosopher Pythagoras (c. 570 – c. 495 BC) had nothing to do with Pythagorean Expectation. Rather, another legend of our time and one of the founding fathers of sports analytics, Bill James, came up with this for the sport of baseball.\nRather than explaining Pythagorean Expectation myself, why not consult Wikipedia:\n Pythagorean expectation is a sports analytics formula devised by Bill James to estimate the percentage of games a baseball team “should” have won based on the number of runs they scored and allowed. Comparing a team’s actual and Pythagorean winning percentage can be used to make predictions and evaluate which teams are over-performing and under-performing. The name comes from the formula’s resemblance to the Pythagorean theorem.\n James’ formula is as follows:\n\\(win\\ ratio_{baseball} = \\frac{runs\\ scored^{k}}{runs\\ scored^{k}\\ +\\ runs\\ allowed^{k}}\\)\nNote: the k-factor James came up with was 2, but has since been modified to 1.83 to better “fit”\nUsing James’ formula as a blueprint, the GM of the Houston Rockets Daryl Morey too the formula and modified it for basketball, and found that the best fit occurred when k = 13.91, leaving the folowing formula to calculate Pythagorean Expected wins for Basketball:\n\\(win\\ ratio_{basketball} = \\frac{points\\ for^{13.91}}{points\\ for^{13.91}\\ +\\ points\\ against^{13.91}}\\)\nThe output of this formula is then multiplied against games played to give the expected wins for the period analysed. This expected wins value is then compared to the teams actual wins, to determine how much lady luck played a part in the team’s season.\nYou must see a pattern by now. The formula remains the same, while the k factor changes based on the sport.\nDifferent k Factors The k factor changes between sports because of the nature of the sports themselves. Some sports, like basketball, are higher scoring and less likely to be decided by chance, hence a higher k factor.\n  Sport K*    Basketball 13.91  NFL 2.37  NHL 2.15  EPL 1.3    *k may vary based on who has derived an “optimal” k\n How can we use it? Studies have shown that winning more games than your Pythagorean Expectation tends to mean a team will decline the following season, while falling short of expectations in the current year tends to mean a team will improve the following season.\n  The formula in code The below function in R code will be used extensively throughout the series to calculate our expected wins factor, to be applied to a team’s games played during the season.\n# create function to apply the formula calculate_expected_wins \u0026lt;- function(points_for, points_against, k) { expected_wins \u0026lt;- ((points_for^k) / ((points_for^k) + (points_against^k))) } Stay tuned for the first sport in our series, College Basketball!\n ","date":1583625600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583625600,"objectID":"bd9b271f845c6fa3bc532c4e31a1cf7f","permalink":"/blog/pythagorean-expectation-pro-sports/","publishdate":"2020-03-08T00:00:00Z","relpermalink":"/blog/pythagorean-expectation-pro-sports/","section":"post","summary":"Introduction This piece is the first part in a series of posts I will be releasing that will look to analyse how many wins teams should’ve won given their performances over the season and compare them to their actual wins achieved.","tags":["R","sports analytics","pythagorean expectation"],"title":"Applying Pythagorean Expectation to Pro Sports - An Intro","type":"post"},{"authors":null,"categories":["afl","analytics"],"content":"  This is the second installment at my look at the age distributions of each AFL team for the 2020 season. The fist post (which can be found here), looked at the impact that the statistic chosen to report out the age profiles can have, specifically using the mean versus the median. Rightly so, many pointed out in feedback that some of the younger players on lists may not play as much or might not be as important as some of the older talent and as such, their age shouldn’t be given as much weighting as some of the stars of each team.\nThat feedback led to this post, which will look at identifying each team’s top line talent and comparing the talent age distribution.\nTo do this, club Best and Fairest (B\u0026amp;F) results have been analysed to identify the top players at each club. Top 10 B\u0026amp;F data comes from here. There are 181 players in total (Richmond have 11 players because of a tie for 10th spot). Given there has been some player movement in the offseason (six players in the list analysed, most notably Tim Kelly to West Coast finished in their team’s top 10 and then changed clubs), these players have now been placed in their new team’s distribution. Doing this has resulted in some teams having more than 10 players analysed, while others less. See below:\n  Team  Number of Players      Adelaide  9    Brisbane Lions  10    Carlton  10    Collingwood  10    Essendon  10    Footscray  12    Fremantle  8    Geelong  9    Gold Coast  10    GWS  10    Hawthorn  11    Melbourne  10    North Melbourne  10    Port Adelaide  10    Richmond  11    St Kilda  10    Sydney  10    West Coast  11     The Age of Top Talent Using the age of top players at each club to measure the age profile of the club shows us that Geelong’s list is the oldest at the top end, with an average age of 29.2 years. These results highlight a concern for Kangaroos fans, as their club’s top 10 B\u0026amp;F comes in at second on this list at 28.9 years and they missed the finals last year (by a bit too, two games out and in 12th position). Adelaide fans also won’t be too happy given their results last year.\nLess concerning for GWS, Essendon and the Bulldogs (who each made finals last year), their top players are in the bottom half of this age analysis. Could be some happy times ahead for these clubs?\nMelbourne has the youngest list, with an average age of 24.7 years, while St Kilda may see this number rise at the start of the 2021 season after they made a decent push in free agency at the end of the 2019 season.\nWe can also see that there is a negative relationship between the top players age distribution and ladder position at the end of a season - the younger the average age of your teams top 10 B\u0026amp;F, the lower the ladder poisition tended to be (with some exceptions of course as mentioned above).\nIt would be interesting to see the impact of this for more than just the most recent season. Might be one for someone else to pick up where I’ve left off, or for a later post.\nData and code for this project can be found here.\n ","date":1577059200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577059200,"objectID":"43717c487472c8515d88c5d2f9013da7","permalink":"/blog/age-of-best-players/","publishdate":"2019-12-23T00:00:00Z","relpermalink":"/blog/age-of-best-players/","section":"post","summary":"This is the second installment at my look at the age distributions of each AFL team for the 2020 season. The fist post (which can be found here), looked at the impact that the statistic chosen to report out the age profiles can have, specifically using the mean versus the median.","tags":["afl","ggplot2","data visualisation","analytics","statistics"],"title":"Playing List Ages at the Pointy End","type":"post"},{"authors":null,"categories":["R","open source","programming","fitzRoy"],"content":"  For the last year or so, I’ve had this desire to contribute to an open source R package, but like a lot of people, I found the thought of tackling the task frightening.\nWhile I work in a really dynamic and close team every day, and in the world of remote repositories (Git), I’ve had really limited exposure to collaborative working in these remote repositories… We tend more to work on projects largely on our own, so the concepts of pull requests (PRs), merging, forking… well it was all a bit daunting.\nThe following is a glimpse at the journey, and will be explained in this post:\nWhat this post won’t be is an exhaustive step-by-step guide of every touch point, rather a medium-high level summary.\n Life is not meant to be easy, my child; but take courage: it can be delightful. - George Bernard Shaw\n With George’s words in mind, I thought time to push myself to jump in.\nScrolling through Twitter (as one does when nursing a newborn), I came across a tweet about a package I’ve used in a few analyses on Don’t Blame the Data that said that the package was now live on CRAN (a great achievement).\nThis naturally led me to the repository on github, at which point I noticed there were open “Issues”, and one of these being for a function to create a ladder for any round.\nThe fitzRoy Package The fitzRoy package, created by James Day, is a package designed to help R users extract and analyse Australian Football League (AFL) data for both the men’s and women’s competitions:\n The goal of fitzRoy is to provide a set of functions that allows for users to easily get access to AFL data from sources such as afltables.com and footywire.com. There are also tools for processing and cleaning that data.\n While I certainly haven’t done any extensive analysis on this point, I would guess that a large proportion of all AFL data analytics projects are completed with the help of this package.\n Jumping right in So rather than think about how good it would be to contribute, why not just get in touch with James and offer to address the open issue…\nJames was super easy to deal with, and boy was he helpful (and patient with this bumbling fool).\nThen came the time to write the function. Well sort of write the function. Fortunately, I had already written this function for a linear regression model I built for predicting the attendance of AFL home and away games here. The function was aptly named return_ladder()… I’m a Data Scientist, not a poet.\nThe function was modified somewhat though to take advantage of the get_match_results() function in the package to return the starting data frame for return_ladder When writing the function, I wanted to address the requirement that the ladder be returned for any round, and for it to be returned for even earlier than the 2011 season, which another API already offered.\nWith that in mind, the function written takes in three arguments, all of which have the option of being blank, as well as specified:\n match_results_df - A data frame extracted using get_match_results(), season_round - The round of the season the user wants the ladder for, season - The season the ladder is required for.  If these are all left blank, the function will return the ladder for every round of every season since the 1897 season.\nHaving the function written was one thing, it also required roxygen notes, that are returned to the user in the help docs of the function. Hadley’s R Packages book does a good job explaining these.\n I’m ready to be a contributor I’ve written the function, the help docs, and have checked the package using devtools::check() to make sure I haven’t made any mistakes that would cause the package to fail it’s build… Nothing looks alarming (well there are some warnings about No visible binding for global variable or something but I’m sure there’s nothing to worry about), but all looked good to me.\nMy local changes were committed and a PR was made, I’m ready to be a contributor, and then bam! Failed codecov!! What is that?! An email to James and I’m told it’s because there were no tests written. Ok cool, I’ll write some tests… WHAT ARE TESTS?! HOW DO I WRITE THESE TESTS?! I found this post to be really helpful, as well as Hadley’s tests in the R Packages book.\nOnce these tests were written, I commit my changes, I’m ready to be a contributor, and then bam! Changes have been made to the master that I haven’t got in my PR… ok so I need to merge the master in my PR - easy (for some maybe, I have no idea). A bit of googling, seems pretty easy, but after typing git merge origin/master, I get this editor pop up in terminal:\nMy initial thoughts? What the is this?!\nBit of googling, ok, it’s a VIM editor. Easy. Write a commit message and then all should be good… WAIT?! How do I get out of this screen?! Bit more googling and after typing :WQ, we’re ready to rock.\n What. Am. I. Doing?!! Ok so things were looking good. I’d committed my changes, all checks passed, happy days.\nYou know that line I had earlier about well there are some warnings about ‘No visible binding for global variable’ or something but I’m sure there’s nothing to worry about?? Well that was nagging away at me, because as James had advised, these would cause issues when trying to include the update on CRAN. So I fixed those, and also updated the Men’s vignette. It’s at this point that I’m a bit hazy on what I did, but all I know is is that I must have spun myself into a Git web…\nThe Master of my forked repo was two commits behind my branch Ladder, which was five commit’s ahead of Origin/Master. What. Am. I. Doing?! Trial and error, error and trial. After much heartache (I can’t stress enough how much heartache), eventually, I got myself all sorted, created another PR and… SUCCESS!!!\nFinally I can say I have successfully made my first contribution to an open source project. I hope that users of this package find the function useful and as with everything, can find improvements to make it even better.\n A quick look at the function The below code gives a glimpse into how the function can be used.\n#----- Install and Load Package -----# # devtools::install_github(\u0026quot;jimmyday12/fitzRoy\u0026quot;) library(fitzRoy) library(tidyverse) library(kableExtra) # get a data frame of AFL data using get_match_results afl_data \u0026lt;- get_match_results() Return the ladder for all teams, for all rounds since 1897\n# apply the return_ladder function ladder \u0026lt;- return_ladder(match_results_df = afl_data) head(ladder, 16) %\u0026gt;% kable(format = \u0026quot;html\u0026quot;, escape = F) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;) %\u0026gt;% scroll_box(width = \u0026quot;750px\u0026quot;, height = \u0026quot;600px\u0026quot;)   Season  Team  Round.Number  Season.Points  Score.For  Score.Against  Percentage  Ladder.Position      1897  Fitzroy  1  4  49  16  3.0625000  1    1897  Collingwood  1  4  41  16  2.5625000  2    1897  Essendon  1  4  47  24  1.9583333  3    1897  Melbourne  1  4  44  27  1.6296296  4    1897  Sydney  1  0  27  44  0.6136364  5    1897  Geelong  1  0  24  47  0.5106383  6    1897  St Kilda  1  0  16  41  0.3902439  7    1897  Carlton  1  0  16  49  0.3265306  8    1897  Fitzroy  2  8  115  42  2.7380952  1    1897  Melbourne  2  8  108  46  2.3478261  2    1897  Collingwood  2  8  91  46  1.9782609  3    1897  Essendon  2  4  77  74  1.0405405  4    1897  Sydney  2  4  67  80  0.8375000  5    1897  Carlton  2  0  52  89  0.5842697  6    1897  St Kilda  2  0  42  107  0.3925234  7    1897  Geelong  2  0  43  111  0.3873874  8      Return the ladder for round 1 for all teams since 1897\n# what if we want the ladder for a specific round? ladder_round_1 \u0026lt;- return_ladder(match_results_df = afl_data, season_round = 1) tail(ladder_round_1, 18) %\u0026gt;% kable(format = \u0026quot;html\u0026quot;, escape = F) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;) %\u0026gt;% scroll_box(width = \u0026quot;750px\u0026quot;, height = \u0026quot;600px\u0026quot;)   Season  Team  Round.Number  Season.Points  Score.For  Score.Against  Percentage  Ladder.Position      2019  GWS  1  4  112  40  2.8000000  1    2019  Fremantle  1  4  141  59  2.3898305  2    2019  Brisbane Lions  1  4  102  58  1.7586207  3    2019  Hawthorn  1  4  87  55  1.5818182  4    2019  Richmond  1  4  97  64  1.5156250  5    2019  Port Adelaide  1  4  87  61  1.4262295  6    2019  Footscray  1  4  82  65  1.2615385  7    2019  Geelong  1  4  72  65  1.1076923  8    2019  St Kilda  1  4  85  84  1.0119048  9    2019  Gold Coast  1  0  84  85  0.9882353  10    2019  Collingwood  1  0  65  72  0.9027778  11    2019  Sydney  1  0  65  82  0.7926829  12    2019  Melbourne  1  0  61  87  0.7011494  13    2019  Carlton  1  0  64  97  0.6597938  14    2019  Adelaide  1  0  55  87  0.6321839  15    2019  West Coast  1  0  58  102  0.5686275  16    2019  North Melbourne  1  0  59  141  0.4184397  17    2019  Essendon  1  0  40  112  0.3571429  18      Return the ladder for every round of the 2018 season\n# finally, for every round in just one season ladder_2018 \u0026lt;- return_ladder(match_results_df = afl_data, season = 2018) head(ladder_2018, 18) %\u0026gt;% kable(format = \u0026quot;html\u0026quot;, escape = F) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;) %\u0026gt;% scroll_box(width = \u0026quot;750px\u0026quot;, height = \u0026quot;600px\u0026quot;)   Season  Team  Round.Number  Season.Points  Score.For  Score.Against  Percentage  Ladder.Position      2018  GWS  1  4  133  51  2.6078431  1    2018  Port Adelaide  1  4  110  60  1.8333333  2    2018  Hawthorn  1  4  101  67  1.5074627  3    2018  Gold Coast  1  4  55  39  1.4102564  4    2018  Sydney  1  4  115  86  1.3372093  5    2018  St Kilda  1  4  107  82  1.3048780  6    2018  Richmond  1  4  121  95  1.2736842  7    2018  Essendon  1  4  99  87  1.1379310  8    2018  Geelong  1  4  97  94  1.0319149  9    2018  Melbourne  1  0  94  97  0.9690722  10    2018  Adelaide  1  0  87  99  0.8787879  11    2018  Carlton  1  0  95  121  0.7851240  12    2018  Brisbane Lions  1  0  82  107  0.7663551  13    2018  West Coast  1  0  86  115  0.7478261  14    2018  North Melbourne  1  0  39  55  0.7090909  15    2018  Collingwood  1  0  67  101  0.6633663  16    2018  Fremantle  1  0  60  110  0.5454545  17    2018  Footscray  1  0  51  133  0.3834586  18      I will be writing a follow up post analysing the AFL ladder through history to really test the function out!\nStay tuned.\n ","date":1576454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576454400,"objectID":"41f555445dd0289309c9ac5cad457680","permalink":"/blog/first-open-source-contribution/","publishdate":"2019-12-16T00:00:00Z","relpermalink":"/blog/first-open-source-contribution/","section":"post","summary":"For the last year or so, I’ve had this desire to contribute to an open source R package, but like a lot of people, I found the thought of tackling the task frightening.","tags":["R","R programming","open source","fitzRoy"],"title":"The Agony and the Ecstasy of my first open source contribution","type":"post"},{"authors":null,"categories":["afl","analytics"],"content":" After seeing my beloved Hawthorn Hawks tweet out an article on their website regarding player ages for each team, it got me riled up that the media love to cite Champion Data’s “average age” as their measure.\nAs can be seen with the age distribution of players as at the start of the 2020 AFL Premiership Season, the figure is going to be skewed by the older players on lists, especially with guys like Shaun Burgoyne, Kade Simpson, Gary Ablett Junior…\n\nMean, Median…what of it? The mean, from Wikipedia;\n For a data set, the arithmetic mean, also called the mathematical expectation or average, is the central value of a discrete set of numbers: specifically, the sum of the values divided by the number of values.\n The median, also Wikipedia;\n The median is the value separating the higher half from the lower half of a data sample (a population or a probability distribution). For a data set, it may be thought of as the “middle” value. For example, in the data set {1, 3, 3, 6, 7, 8, 9}, the median is 6, the fourth largest, and also the fourth smallest, number in the sample. For a continuous probability distribution, the median is the value such that a number is equally likely to fall above or below it.\n When data is not normally distributed around the mean (as is the case here), using it to describe the measure of centre is misleading. In cases like this, the median provides a more representative statistic. With a positively skewed distribution like the one above, statistics 101 tells us that the median will be less than the average, while the opposite would hold true where the distribution was negatively skewed.\nThe average age of all players on team lists for the 2020 AFL premiership season is 24.17 years, while the median age is 23.6 years.\n\nSo why do they still report averages? I suspect media outlets report the average age for two reasons;\nWho can be bothered explaining the nuances between the mean and the median; or more likely, The average is more prone to pull this number higher for some teams, fuelling the narrative they want to run with.  \n Implications of the different measures When we plot both the average and median ages of each of the teams, we can see that we get some very different outcomes.\nWe are led to believe that the Geelong Cats have the fourth oldest list but when using the median as the statistic, they are the 11th oldest playing list, with the Kangaroos, GWS, Saints, Demons, Bombers, Bulldogs and Tigers all having older lists than them.\nSome of the other implications:\n The Brisbane Lions have the youngest list for the 2020 season, not the Gold Coast The Hawks are actually the fourth oldest list, with North Melbourne being third The Demons are the the 7th oldest list, not the 12th oldest The Power are the equal third (with the Swans) youngest list, not the 10th oldest  \n  Why does any of this matter? How this figure is reported probably doesn’t matter ultimately; the age of a playing list likely isn’t all that important when it comes to the ultimate glory of premiership success, as can be seen in the different age distributions of recent premiership teams in this great little analysis at TheArc.\nThat aside, it would be nice to see this figure reported a little more accurately.\nRant. Over.\nThe data for this post and the code used to scrape it can be found here.\n ","date":1575417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575417600,"objectID":"1fa5bf877bd970b033ea1acada4e9674","permalink":"/blog/analysing-afl-team-age/","publishdate":"2019-12-04T00:00:00Z","relpermalink":"/blog/analysing-afl-team-age/","section":"post","summary":"After seeing my beloved Hawthorn Hawks tweet out an article on their website regarding player ages for each team, it got me riled up that the media love to cite Champion Data’s “average age” as their measure.","tags":["afl","ggplot2","data visualisation","analytics","statistics"],"title":"Analysing AFL Team Age... Properly!","type":"post"},{"authors":null,"categories":["R","gganimate","ggplot2","data visualisation","learning"],"content":" This post aims to introduce you to animating ggplot2 visualisations in r using the gganimate package by Thomas Lin Pedersen.\nThe post will visualise the theoretical winnings I would’ve had, had I followed the simple model to predict (or tip as it’s known in Australia) winners in the AFL that I explained in this post. The data used in the analysis was collected from the AFL Tables website as part of a larger series I wrote about on AFL crowds. The wider project can be found here\nlibrary(tidyverse) library(lubridate) library(scales) library(gganimate) # set themes theme_set(theme_minimal() + theme(axis.title.x = element_text(size = 16, hjust = 1), axis.title.y = element_text(size = 16), axis.text = element_text(size = 13), plot.title = element_text(size = 19))) # create a colour pallette colours \u0026lt;- c(\u0026quot;#5EB296\u0026quot;, \u0026quot;#4E9EBA\u0026quot;, \u0026quot;#F29239\u0026quot;, \u0026quot;#C2CE46\u0026quot;, \u0026quot;#FF7A7F\u0026quot;, \u0026quot;#4D4D4D\u0026quot;) #----- Read in data -----# afl \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/JaseZiv/AFL-Crowd-Analytics/master/data/cleaned_data/afl_model_data.csv\u0026quot;, stringsAsFactors = F) # Data pre-processing ----------------------------------------------------- # make all variables character type to make splitting and string manipulation easier afl \u0026lt;- afl %\u0026gt;% mutate_if(is.factor, as.character) %\u0026gt;% mutate(team1_score = as.numeric(team1_score), team2_score = as.numeric(team2_score)) %\u0026gt;% mutate(fav_team = ifelse(AwayLineClose \u0026lt; 0, team2, team1)) %\u0026gt;% mutate(winning_team = ifelse(winner == \u0026quot;Home\u0026quot;, team1, ifelse(winner == \u0026quot;Away\u0026quot;, team2, \u0026quot;Draw\u0026quot;))) %\u0026gt;% mutate(fav_win = ifelse(fav_team == winning_team, \u0026quot;Yes\u0026quot;, \u0026quot;No\u0026quot;)) %\u0026gt;% filter(season \u0026gt;= 2014, !str_detect(round, \u0026quot;F\u0026quot;)) %\u0026gt;% mutate(tip = ifelse(abs(AwayLineClose) \u0026lt; 3, team1, fav_team)) # function to calculate odds calculate_odds_available \u0026lt;- function(tip, winning_team, team1, team2, HomeOddsClose, AwayOddsClose) { if(tip == winning_team) { odds_available \u0026lt;- ifelse(tip == team1, HomeOddsClose, AwayOddsClose) } else { odds_available \u0026lt;- 0 } } # apply function and calculate returns afl \u0026lt;- afl %\u0026gt;% mutate(odds_available = mapply(calculate_odds_available, tip, winning_team, team1, team2, HomeOddsClose, AwayOddsClose), game_return = odds_available * 10, game_profit = game_return - 10) # create a df that calculates total winnings per round money_per_round \u0026lt;- afl %\u0026gt;% group_by(season, round) %\u0026gt;% summarise(total_profit = sum(game_profit)) %\u0026gt;% ungroup() # add a round 0, where all seasons start at $0 zeros \u0026lt;- data.frame(season = (2014:2019), round = 0, total_profit = 0) # join zeros df on to money_per_round money_per_round \u0026lt;- money_per_round %\u0026gt;% rbind(zeros) # create a df that sums up winnings cumulatively total_money \u0026lt;- money_per_round %\u0026gt;% arrange(season, round) %\u0026gt;% group_by(season) %\u0026gt;% mutate(cumulating_winnings = cumsum(total_profit)) %\u0026gt;% ungroup() Let’s start Ok, so the first step I took when writing the original post was to create a ggplot2 visual to plot the winnings (or losses) I would’ve made using my simple strategy.\nThis was the result:\ntotal_money %\u0026gt;% ggplot(aes(x= round, y= cumulating_winnings, group = season, colour = as.character(season))) + geom_line(size = 1) + geom_point(size = 2, colour = \u0026quot;black\u0026quot;) + labs(x= \u0026quot;Round\u0026quot;, y= \u0026quot;Cumulative Wins/Losses\u0026quot;, colour = \u0026quot;Season\u0026quot;) + scale_x_continuous(limits = c(0, 27), labels = c(0, 3, 6, 9, 12, 15, 18, 21, 24), breaks = c(0, 3, 6, 9, 12, 15, 18, 21, 24)) + scale_colour_manual(values = colours) + ggtitle(\u0026quot;2017 WOULD\u0026#39;VE BEEN A BAD YEAR\u0026quot;) + theme(legend.position = \u0026quot;bottom\u0026quot;) Not bad, but certainly could be improved. To my mind, with six seasons being plotted, the legend is hard to map to the line itself. Also, other than the 2017 season, which was particularly bad, the other seasons’ variation between rounds was hard to see.\nAdditionally, plotting the data this way made it hard to realise without expending far too much energy focusing on where I would’ve made money, and where I would’ve lost it.\n Labels and Annotations Yuck - you can’t just simply add the season as a label - you can’t read anything!\ntotal_money %\u0026gt;% ggplot(aes(x= round, y= cumulating_winnings, group = season, colour = as.character(season))) + geom_line(size = 1) + geom_point(size = 2, colour = \u0026quot;black\u0026quot;) + geom_hline(yintercept = 0, linetype = 2, size = 2) + # added in horizontal line at $0 geom_text(aes(label = season), hjust = -1, size = 6) + # season labels added scale_colour_manual(values = colours) + labs(x= \u0026quot;Round\u0026quot;, y= \u0026quot;Cumulative Wins/Losses\u0026quot;) + scale_x_continuous(limits = c(0, 27), labels = c(1, 3, 6, 9, 12, 15, 18, 21, 24), breaks = c(1, 3, 6, 9, 12, 15, 18, 21, 24)) + scale_y_continuous(labels = dollar) + # y-axis formatted to dollar format using scales annotate(\u0026quot;text\u0026quot;, x= 26, y= 6, label = \u0026quot;Break Even $\u0026quot;, size = 6) + # added text to break even line ggtitle(\u0026quot;2017 WOULD\u0026#39;VE BEEN A BAD YEAR\u0026quot;) + theme(legend.position = \u0026quot;none\u0026quot;) # turned legend off Instead, only one season label was applied, and applied at the end of each line’s run. This looks much better.\nAs we can see, further elements were added to our static chart, including:\n Adding the break-even line; Formatting the y-axis to a dollar format; and Adding labels and removing the legend  This has greatly improved the readability of the plot.\ntotal_money %\u0026gt;% ggplot(aes(x= round, y= cumulating_winnings, group = season, colour = as.character(season))) + geom_line(size = 1) + geom_point(size = 2, colour = \u0026quot;black\u0026quot;) + geom_hline(yintercept = 0, linetype = 2, size = 2) + # added in horizontal line at $0 geom_text(data = total_money %\u0026gt;% filter(round == max(round)), aes(label = season), hjust = -0.3, size = 6) + # season labels added, but only one label per season scale_colour_manual(values = colours) + labs(x= \u0026quot;Round\u0026quot;, y= \u0026quot;Cumulative Wins/Losses\u0026quot;) + scale_x_continuous(limits = c(0, 27), labels = c(1, 3, 6, 9, 12, 15, 18, 21, 24), breaks = c(1, 3, 6, 9, 12, 15, 18, 21, 24)) + scale_y_continuous(labels = dollar) + # y-axis formatted to dollar format using scales annotate(\u0026quot;text\u0026quot;, x= 26, y= 6, label = \u0026quot;Break Even $\u0026quot;, size = 6) + # added text to break even line ggtitle(\u0026quot;2017 WOULD\u0026#39;VE BEEN A BAD YEAR\u0026quot;) + theme(legend.position = \u0026quot;none\u0026quot;) # turned legend off  Hello Animations! While the above chart looks a lot better, there are no theatrics!\nEnter animations from gganimate!\nUsing an animated plot allows us to remove even more elements. With the right mix of labelling and animation, the y-axis no longer is necessary - with each round, we can follow the winnings or losses as we go, while the break-even line gives us a reference point.\nThe other things that were done here include the slowing down of frames using fps (frames per second) and adjusting the range in transition_reveal() to be longer than the rounds it’s transitioning over (ie adjusting the range to c(0,25)). This allows the animation to pause after it has finished its cycle.\nFinally, to increase the size of the output, adjust the width and height arguments to your liking.\ntotal_money_anim \u0026lt;- total_money %\u0026gt;% ggplot(aes(x= round, y= cumulating_winnings, group = season, colour = as.character(season))) + geom_line(size = 2) + geom_point(size = 3, colour = \u0026quot;black\u0026quot;) + geom_hline(yintercept = 0, linetype = 2, size = 2) + geom_text(aes(label = paste0(season, \u0026quot;: \u0026quot;, dollar(cumulating_winnings))), hjust = -0.3, size = 6) + scale_colour_manual(values = colours) + labs(x= \u0026quot;Round\u0026quot;, y= \u0026quot;Cumulative Wins/Losses\u0026quot;) + scale_x_continuous(limits = c(0, 27), labels = c(1, 3, 6, 9, 12, 15, 18, 21, 24), breaks = c(1, 3, 6, 9, 12, 15, 18, 21, 24)) + scale_y_continuous(labels = dollar) + annotate(\u0026quot;text\u0026quot;, x= 26, y= 6, label = \u0026quot;Break Even $\u0026quot;, size = 6) + ggtitle(\u0026quot;2017 WOULD\u0026#39;VE BEEN A BAD YEAR\u0026quot;) + theme(legend.position = \u0026quot;none\u0026quot;, axis.text.y = element_blank(), axis.title.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank()) + transition_reveal(round, range = c(0, 25)) animate(total_money_anim, width = 900, height = 900, fps = 5) Hope this has given you some inspiration to go out and start producing some animated visualisations of your own.\nLet me know in the comments if you have any questions or suggestions.\n ","date":1567123200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567123200,"objectID":"bcac002591ac4c38d0350e10f1ec350a","permalink":"/blog/all-roads-lead-to-gganimate/","publishdate":"2019-08-30T00:00:00Z","relpermalink":"/blog/all-roads-lead-to-gganimate/","section":"post","summary":"This post aims to introduce you to animating ggplot2 visualisations in r using the gganimate package by Thomas Lin Pedersen.\nThe post will visualise the theoretical winnings I would’ve had, had I followed the simple model to predict (or tip as it’s known in Australia) winners in the AFL that I explained in this post.","tags":["gganimate","ggplot2","data visualisation","R","afl","line chart"],"title":"All roads lead to gganimate","type":"post"},{"authors":null,"categories":["afl","R","ggplot2","gganimate"],"content":" I feel like I always overthink footy tipping. During each round, I make myself believe I have some sort of secret sauce and conjure up visions in my head of nailing a solid roughy… and then fall flat half way through the season and give up…\nSo then I thought, surely there’s an easier way. Only problem was, I thought this up last weekend (during the last season of the round)…I wonder how many tips I would’ve gotten this year had I just picked the home team every game…\nOk, so 113 winners wouldn’t have been good enough…\nUsing the Market Then I thought, what if I just picked the favourites in the betting market for each game, but with a twist? If the line was less than half a goal (3 points - virtually a coin flip), then I’d just go safe and pick the home team.\nThe betting odds data comes from the Australian Sports Betting website which uses Bet365 odds.\nWow!!\nOk, so I would’ve been able to (theoretically) get my tips up to 135 for the 2019 season - a really handy result, and beating some popular machine learning models that are out there! This 135 correct tips also doesn’t count draws being awarded as a point, as some tipping competitions do (although there were no draws in 2019).\nOther than 2017, this model - I’ll refer to it as “Simple Model” - would’ve been fairly competitive in each season since 2014.\nMaybe next year, I’ll know what I’ll employ… my Simple Model!\nBut can we make some money Ok, so now that I know that the Simple Model’* performs fairly competitively in tipping comps, I want to know if I can make some money using this method?\nTo answer this question, I’m placing a theoretic $10 on each game and seeing how many bags of cash are left at the end.\nHmmm… so using this method, I would’ve lost almost $40, even though it was good enough to win some tipping comps.\nOnly in 2018 would this method have worked, with a net profit of $26.82…\nThe 2017 season would’ve been the most brutal, losing just over $245 for the season.\nTo find out where everything went so wrong, we can use an animated line plot that tracks the overall profit throughout each round of the season. To get a rundown on how the animated plot was generated, see this post here.\nDuring the 2019 season, we can see that things would’ve started off really bleak - after round five, I would’ve been down over $120! Things started to pick up from there though, even being in the black at round 14. At no point during the 2015 and 2017 seasons would this model have been profitable.\nWhile keeping me competitive in footy tipping comps, this model certainly isn’t going to allow me to retire anytime soon.\n  ","date":1567123200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567123200,"objectID":"2fb1a2e506beaef7b025138c84b26dbc","permalink":"/blog/simplifying-afl-tipping/","publishdate":"2019-08-30T00:00:00Z","relpermalink":"/blog/simplifying-afl-tipping/","section":"post","summary":"I feel like I always overthink footy tipping. During each round, I make myself believe I have some sort of secret sauce and conjure up visions in my head of nailing a solid roughy… and then fall flat half way through the season and give up…","tags":["afl","ggplot2","data visualisation","R","gganimate"],"title":"Effectively Simplifying AFL Tipping","type":"post"},{"authors":null,"categories":["R","sport","analytics","twitter"],"content":" On the 24th of August, 2019, the Australian Men’s Basketball team, the Boomers, created history when they were able to enact their own David and Golliath moment, taking down Team USA for the first time ever. More impressively was the fact that the US hadn’t lost a game in 13 years. It was a marvelous moment in Australian basketball history, and while the US fielded what is widely considered one of their weaker teams, it was a moment for us to all savour. The fact that we were able to bring the best country in the world to play some exhibition games should have been celebrated, regardless of the result.\nThat was not to be the case…\nAfter the historic first men’s basketball game at Marvel Stadium, the narrative from the media appeared to be full of negativity, predominantly focusing on the expensive seating with poor views for fans.\nI suspect that this happens everywhere, but we Aussies seem to let our opinions swing more than most - we’re more than happy to rag on individuals or teams when things aren’t tracking so well, but flip pretty quickly when the are (Nick Kyrgios says hi).\nThis analysis will look at the Twitter activity around the time of both games in an attempt to prove or disprove the narrative change.\nTo collect tweet data, the rtweet and ROAuth packages were used. Tweets between the 22nd August and 2pm on the 25th of August were collected and analysed in the blow analysis. Tweets using the official hashtag - #BoomersUSA are referencing @BasketballAus were scraped.\nThe full code base and data for this project can be found here\nIf the tweet was recorded after 2019-08-24 04:00:00 UTC (tip-off time for game 2), it was classed as a tweet in Game2 Starts. This allows us to compare tweets leading in to the historic second game with those that occurred during and after the magical night.\nTweet Analysis Looking at all tweets since game 1, we can see that game 2 had more tweets per hour. No doubt the shock result played a massive part in this.\nWith 4019 ‘favourites’, the following tweet from NBATV was the most favoured tweet:\n It feels awesome. … I hope we can all build on this.” @Patty_Mills describes his emotions after leading @BasketballAus to its first ever win over the U.S.\n The most retweeted tweet from the period analysed came from NBL and had 833 retweets. The tweet was:\n Patty Mills in the 4th quarter is a piece of art 🔥\u000b#BoomersUSA @SBSVICELAND @SBSOnDemand\n  Tweet Words Used Before we can measure the sentiments of tweets, the tweet strings need to be split into ‘tokens’ (or individual words).\nOnce these tokens have been unnested (split out), we can plot the most frequently used words. Importantly, stop-words and other words we don’t want in our analysis have been omitted. Stop-words include “and”, “the”, “a”, etc - words that don’t add a lot to a sentiment analysis. Additionally, “BoomersUSA” was removed, as this was the hashtag for the game and was mentioned in almost all tweets.\nThe 20 most frequently used words for tweets that occured before and after game 2 are plotted below.\nAs expected, “seats”, “plastic”, “seating” were words that frequently appeared in tweets before Game 2, where only “seats” appeared in tweets after Game 2 started. For tweets after game 2 started, “history”, “Patty Mills”, “awesome” and “love” all appeared in tweets frequently - very soft and mushy hey?\nThe plot below allows us to get an even better look at the differences between the words used before and after Game 2. Words to the bottom right of the diagonal line running through the plot indicate words more frequently used in tweets before game two, while words above the line were more frequently used during and after Game 2.\nLooks like we were a happy bunch finally…\n Tweet Sentiment Analysis Once the tokens have been separated, a sentiment score can be calculated.\nThe method that will be used is the common lexicon for sentiment analysis created by Finn Årup Nielsen (http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010), called the AFINN lexicon. Words that are more positive (say, “awesome” for example) are given positive score further from zero than more negative words (like “devastated”), which are given socres further below zero.\nTo get a feel for the power of sentiment analysis, the following tweet was the most positive tweet, with a positivity index of 17:\n Totally thrilled with the Boomers win today, and Patty Mills was just brilliant when it counted. Really wonderful team effort. Bring on the World Cup! #BoomersUSA\n At the (complete) opposite end of the spectrum, the following tweet was the most negative, with a score of -18 (sorry for the profanities, I’ve done my best to clean them out):\n Who the f!\u0026amp;% hired these people to organise this event, what a f!\u0026amp;%ing bs stich up. $ chairs from Bunnings does that come with a forking snag c!\u0026amp;%. Like WTF surely the company who funding to set up the stadium is not that broke.#BoomersUSA\n No surprises, the most positive tweet was after we won, the most negative after Game 1.\nPlotting the distribution of sentiment scores for each tweet, we can clearly see that the tweets after game 2 started became considerably more positive - the median positivity score (the ratio of positive to negative words) for these tweets was 0.95, over doube the 0.43 median for tweets prior to game 2.\nAs I suspected, we became much happier after our historic win… almost to the point where we’d forgotten about the seating “debacle”, even though the second game was played at the exact venue, with the exact seating arrangement… very strange.\nI might put this theory of us Aussies to the ultimate test one day and see how we respond to Kyrgios’ success… if he ever tastes the ultimate success!\nFeel free to leave some feedback if you like in the comments below.\n ","date":1566691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566691200,"objectID":"c65e193ed75f0b621645203df81bd9b8","permalink":"/blog/how-the-narrative-changed-after-the-boomers-win/","publishdate":"2019-08-25T00:00:00Z","relpermalink":"/blog/how-the-narrative-changed-after-the-boomers-win/","section":"post","summary":"On the 24th of August, 2019, the Australian Men’s Basketball team, the Boomers, created history when they were able to enact their own David and Golliath moment, taking down Team USA for the first time ever.","tags":["boomers","USA","teamusa","twitter","sentiment","patty mills","basketball"],"title":"How the narrative changed after the Boomers' win","type":"post"},{"authors":null,"categories":["R","gaming","analytics","tidyverse","ggplot2"],"content":"  This post was originally released on the Kaggle FIFA 19 Complete Player dataset that was kindly collected by Karan Gadiya. Many thanks Karan. The link to my original kernel is here\nThe full code base can be found here\nIntroduction From Wikipedia:\n FIFA 19 is a football simulation video game developed by EA Vancouver as part of Electronic Arts’ FIFA series. Announced on 6 June 2018 for its E3 2018 press conference, it was released on 28 September 2018 for PlayStation 3, PlayStation 4, Xbox 360, Xbox One, Nintendo Switch, and Microsoft Windows. It is the 26th installment in the FIFA series. As with FIFA 18, Cristiano Ronaldo appears as the cover athlete of the regular edition. https://en.wikipedia.org/wiki/FIFA_19\n The game features a number of different playing modes, however Career mode as a manager holds the most appeal for me.\nThe following analysis will be tailored toward having the best chance at success in that mode for anyone interested.\nSome things I want to analyse in this paper:\n High level Exploratory Data Analysis Which features are highly correlated with a player’s overall rating by player position Analyse the differences between a player’s current rating and their potential rating Find out which teams have the highest potential Find out the youngest teams / oldest teams Use k-means clustering to try to find “bargains”; ie if there is someone with the same skills/potential, can they be found for a bargain?   Feature engineering # Load libraries library(tidyverse) library(scales) library(ggthemes) library(kableExtra) library(plotly) options(scipen = 999) fifa_data \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/JaseZiv/FIFA19-Analysis/master/data/data.csv\u0026quot;) %\u0026gt;% select(-X1) fifa_data \u0026lt;- fifa_data %\u0026gt;% mutate(ValueMultiplier = ifelse(str_detect(Value, \u0026quot;K\u0026quot;), 1000, ifelse(str_detect(Value, \u0026quot;M\u0026quot;), 1000000, 1))) %\u0026gt;% mutate(ValueNumeric_pounds = as.numeric(str_extract(Value, \u0026quot;[[:digit:]]+\\\\.*[[:digit:]]*\u0026quot;)) * ValueMultiplier) %\u0026gt;% mutate(Position = ifelse(is.na(Position), \u0026quot;Unknown\u0026quot;, Position)) fifa_data \u0026lt;- fifa_data %\u0026gt;% mutate(WageMultiplier = ifelse(str_detect(Wage, \u0026quot;K\u0026quot;), 1000, ifelse(str_detect(Wage, \u0026quot;M\u0026quot;), 1000000, 1))) %\u0026gt;% mutate(WageNumeric_pounds = as.numeric(str_extract(Wage, \u0026quot;[[:digit:]]+\\\\.*[[:digit:]]*\u0026quot;)) * WageMultiplier) positions \u0026lt;- unique(fifa_data$Position) gk \u0026lt;- \u0026quot;GK\u0026quot; defs \u0026lt;- positions[str_detect(positions, \u0026quot;B$\u0026quot;)] mids \u0026lt;- positions[str_detect(positions, \u0026quot;M$\u0026quot;)] f1 \u0026lt;- positions[str_detect(positions, \u0026quot;F$\u0026quot;)] f2 \u0026lt;- positions[str_detect(positions, \u0026quot;S$\u0026quot;)] f3 \u0026lt;- positions[str_detect(positions, \u0026quot;T$\u0026quot;)] f4 \u0026lt;- positions[str_detect(positions, \u0026quot;W$\u0026quot;)] fwds \u0026lt;- c(f1, f2, f3, f4) fifa_data \u0026lt;- fifa_data %\u0026gt;% mutate(PositionGroup = ifelse(Position %in% gk, \u0026quot;GK\u0026quot;, ifelse(Position %in% defs, \u0026quot;DEF\u0026quot;, ifelse(Position %in% mids, \u0026quot;MID\u0026quot;, ifelse(Position %in% fwds, \u0026quot;FWD\u0026quot;, \u0026quot;Unknown\u0026quot;))))) fifa_data \u0026lt;- fifa_data %\u0026gt;% mutate(AgeGroup = ifelse(Age \u0026lt;= 20, \u0026quot;20 and under\u0026quot;, ifelse(Age \u0026gt; 20 \u0026amp; Age \u0026lt;=25, \u0026quot;21 to 25\u0026quot;, ifelse(Age \u0026gt; 25 \u0026amp; Age \u0026lt;= 30, \u0026quot;25 to 30\u0026quot;, ifelse(Age \u0026gt; 30 \u0026amp; Age \u0026lt;= 35, \u0026quot;31 to 35\u0026quot;, \u0026quot;Over 35\u0026quot;))))) 1. Player Valuation The raw data has player valuations as a character string, with a designation at the end specifying whether the value is thousands or millions. Regex is used to create a numeric variable called ValueNumeric_pounds\n2. Player Wage See Point 1 above. Same transformation has occurred for player Wage\n3. Player Positions There are 28 different positions in FIFA2019. To make analysis less granular, I have decided to create four groupings; GK, DEF, MID and FWD.\n4. Player Age I have decided to group players into age buckets, in 5 year increments other than for players 20 years and younger, and players 35 years and over.\n Overall Ratings Player ratings are normally distributed in FIFA19, with a mean of 66.2387 and standard deviation of 6.9089\nAge vs Overall Rating Athletes in all domains have no doubt been looking for the elixir of life since the dawn of time, but to no avail unfortunately. When it comes to a player’s overall rating, it appears as though player ratings are growing on average until around 30 years of age, whereby they level off for a couple of years, and then start the inevitable decline at around 34 years.\nWhen this relationship is explored by the major position groups, we can see that defender ratings tend to beging their decline earliest at around 33 years of age, while the decline starts somewher closer to 35 for both attackers and midfielders.\n  Player Valuations Player valuations show a heavily positive skew, being skewed by the superstars of the game - Messi, Neymar, De Bryune, etc.\nAge vs Valuations It is intuitive to think that players get better with age and experience and that their valuations would refelect this relationship.\nPlotting the relationship below, it can be seen that as players age, their valuations tend to increase up to their early 30s, and then begin declining in the years of age between 31-35, and then rapidly decline for players older than 35. This is in line with the findings when the overall rating was plotted as a function of the player’s age.\n Position vs Valuations As expected, Forwards and Midfielders are going to cost you more than Defenders and Goalkeepers.\nSpecifically, it’s left and right forwards and left and right attacking-midfielders that are the most expensive positions.  Player Rating and Valuations There a strong positive relationship between the players rating and valuation with a Spearman correlation of 0.9081. The Spearman method was used to calculate the correlation because of presence of large outliers.\n Correlations with a players Overall rating The below correlation matrix data displays the correlations between the Overall rating and other key attribute variables for all players except goalkeepers. Both the Pearson and Spearman correlation methods were used to display the differences.\nThe Spearman correlation method is more robust with dealing with extreme outliers, hence the player Value having the highest spearman correlation (0.92). Reactions, Composure, Special and (surprisingly) Wage rounding out the top 5 correlated variables with Overall rating using the Spearman method. I say surprisingly because Soccernomics (Kuper.S, Szymanski.S, 2014) stated the best predictor of team success was the teams wage bill.\nUsing the Pearson method, we get different results. the top 5 correlated variables with the Overall rating is Reactions, Composure, Special, ShortPassing and BallControl. Both value and wage don’t appear ni this list because they are prone to large outliers.\n  Feature  Spearman  Pearson      ValueNumeric_pounds  0.9160253  0.6346473    Reactions  0.8430347  0.8477221    Composure  0.7928322  0.8017716    Special  0.7808008  0.7959002    WageNumeric_pounds  0.7786888  0.5755838    BallControl  0.7327561  0.7178017    ShortPassing  0.7209597  0.7226152    Potential  0.6148446  0.6506847    ShotPower  0.5928283  0.5629406    LongPassing  0.5878514  0.5853744      Which positions are skilled in which attributes The heat map below displays the median attributes for each position available. The analysis was done on players other than goalkeepers, and also only on players with an overall rating of 75 or higher.\nIt shows that centre backs are high on strength and are typically strong in agression. Right and Left Wingers and Left Forwards are very agile, while Left and Right Midfielders have great acceleration.\n  Helping me manage a team The next section will be devoted to analysis that will assist those wanting to magae a team. I will analyse which players to target if you want to manage a team in rebuilding mode, players to target with a low budget if managing lower division teams or lower-budget teams, which teams to manage if tyou want an attacking team, or a defensive team.\nYoung Player Analysis If you are wanting to be a manager that gives youngsters a chance and watch them blossom, then the list of below players might be ones to target.\nThe violet bars indicate the difference between the player’s potential and their current overally rating. J. von Moos, a striker (17 years old and costs €280K) and D. Campbell, a central midfielder (also 17 years old and only costs €60K) are the two players with the highest room to grow, with both players having a differential of 26. R. Griffiths, also a striker, but 18 years old, has a potential of 84, however will cost you €575K. I know who I’d be choosing between Griffiths and the previously mentioned J. von Moos who also has an overall rating of 84.\nIf it’s a real bargain you’re looking for, the 16 year old central midfielder B. Mumba will only cost you €190K, but has a potential rating of 80… Juicy!\n When does potential end? This now raises an interesting topic… is there an age that players finally realise all of their potential, or is there room to grow throughout their careers?\nIt appears as though a player’s potential and their overall rating converge at approximately 30 years of age.\n  Free Valuation Players Are there some real bargains to be had? It would appear that there is! Below plots all players that have a free valuation, and displays a player’s age against their current overall rating.\nL. Paredes!!!!! Depending on what you’re looking for, either of the top-left or top-right quadrants are where the players to target are. The best player to target is L. Paredes, a fairly young CM at 24 with an overall rating of 80 (and even better still a potential of 85).\nP. Mahlambi, a South African CAM 20 year old with a potential of 84 is another big target, as is the 18 year old RCB B. Méndez and 19 year old CAM I. Hagi.\n Which Team do I want to manage This section is designed to help future managers decide what team to manage, based on various factors.\nI will include an analysis on the youngest and oldest teams, the most talented teams, the great attacking teams, most expensive teams and teams with the highest player valuations.\nTeam Age If you’re the type of manager who likes to take charge of younger teams, then teams in some of the European teams might be for you. FC Nordsjælland (Danish) is the youngest team on average in FIFA19, with an average age of 20.3 years old, followed by FC Groningen (Dutch) at 21.4 years old and Bohemian FC (Ireland) at 21.5 years old. Interestingly, Dutch powerhouses PSV and Ajax both make the list of the 20 youngest teams. They’re certainly the type of teams I’d be looking towards.\nAt the other end of the spectrum, if managing older teams is your thing, then South America is the place for you.\n Team Overall Talent If you’re trying to pick the most talented team, how you choose to interpret “talent” can have an impact on which team you finally settle for.\nWhen looking at the average overall ratings for each team, four of the five highest average rated teams come from Serie A (Juventus, Napoli, Inter and Milan), with Real Madrid the only non-Italian side in the top 5. FC Barcelona, PSG, Roma, Man Utd and surprisingly SL Benfica rounding out the top 10.\nNow if you wish to define “talent” as the most number of superstars, then your selection will be slightly different.\nI have chosen to define superstars as those players whose rating is 85 or above. There are 110 110 players in the game with this designation.\nLooking at the top three from the previous plot, we can see that it’s a much different story. Juventus - the highest average overall rated team only have the third most amount of superstars, replaced at the top by Real Madrid and Manchester City. The story is even more pronounced for Napoli, who went from second in the previous measurement to 11th here, with Inter also dropping form 3rd to 8th.\nThe Galacticos of Real Madrid proudly sit atop this perch with 12 players on the list having an overall rating of 85 or over.\nInterestingly, only the the top leagues are represented in the list of clubs with more than one superstar.\n Deadly Teams Up Front How does one determine the most attacking teams? The method used in this analysis takes an average rating of the four main attributes related to goals - Finishing, LongShots, Penalties, ShotPower and Positioning. The attributes are added together and an average rating is calculated. To determine the teams with the highest attacking rating, only midfielders and attackers are considered.\nThe average rating of the most dangerous teams in front of goals are displayed below. Inter (12 midfielders and attackers) and Juventus (13 midfielders and attackers) are the most dangerous teams in front of goal, clearly ahead of the third most dangerous club, Napoli (13 midfielders and attackers). Interestingly, there are a few leagues are represented in the 20 most dangerous teams up front - Portugese, Turkish, Greek, in addition to the usual suspects in England, Italy, Germany, Spain, France.\n  Club  NumberOfAttackers  TeamAttackingRating      Inter  12  74.43333    Juventus  13  74.03077    Napoli  13  71.72308    Manchester United  19  71.50526    Sporting CP  17  71.15294    FC Porto  16  71.05000    FC Barcelona  18  71.03333    Real Madrid  17  71.01176    SL Benfica  16  70.87500    FC Bayern München  17  70.82353    SC Braga  15  70.78667    Roma  13  70.60000    Manchester City  16  70.37500    Milan  14  70.27143    Olympique de Marseille  12  70.13333    FC Schalke 04  16  69.12500    Valencia CF  17  69.01176    Beşiktaş JK  14  68.75714    Paris Saint-Germain  17  68.51765    SV Werder Bremen  17  68.45882      Team Wage Bills and Value for Money Real Madrid and Barcelona blow the other teams away when it comes to wage bills, with weekly wage bills sitting at just over €5M and €4.83M respectively. Man City is a distant third with a weekly wage bill of €3.7M.\nSurprisingly, Inter Milan, one of the most highly rated teams in terms of overall talent (see above) comes in at number 16 on the list - talk about value for money! Even more strangely, Premier League sides Everton and West Ham have higher wage bills than Inter! What’s worse for those two EPL clubs, Milan, Napoli and Inter were three of the top five overall rated clubs in the game. As a result, these three clubs have had great value for money, with each overall rating point only costing Inter €20K, as opposed to Everton’s €27K per overall rating. Of course this is dwarfed by the two Spaniards, who both spend over €60K per point.\nAt the other end of the spectrum, there are some clubs that are exceptionally efficient with their wage bills. Highlighted clubs are those that have an average overall rating over 70. Spartak Moscow are the leaders, with each rating point only costing €369 - what a difference from Real Madrid!\n  K-Means Clustering Trying to find players who are similar can be a very difficult task. There are close to 100 variables that you might need to analyse to determine “similarity” between players. Couple that with comparing multiple players and it’s almost imposible.\nEnter K-Means!\nK-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells https://en.wikipedia.org/wiki/K-means_clustering.\nThe steps used are outlined below:\nFilter out Goalkeepers and any players without a position listed Select only numeric variables, mainly the attribute variables. Player value, wages and overall rating have been omitted from the data so that these variables don’t sway our groupings, allowing for the clusters to contain like-for-like players based off their skillsets. Scale the data - without scaling, some variables, especially Special will skew the clusters to than variable. Run K-Means using, looping through a number of cluster centres to find the optimal number (k). This can be done visually, where the elbow of the plot “bends”. Set the number of centres (8 in this example) Re-run the model with K = 8 Assign the cluster group back to the main data  # Get data ready data_cluster \u0026lt;- fifa_data %\u0026gt;% filter(PositionGroup != \u0026quot;Unknown\u0026quot;) %\u0026gt;% filter(PositionGroup != \u0026quot;GK\u0026quot;) %\u0026gt;% mutate(RoomToGrow = Potential - Overall) %\u0026gt;% select_if(is.numeric) %\u0026gt;% select(-ID, -`Jersey Number`, -AttackingRating, -starts_with(\u0026quot;Value\u0026quot;), - starts_with(\u0026quot;Wage\u0026quot;), -starts_with(\u0026quot;GK\u0026quot;), -Overall) scaled_data \u0026lt;- scale(data_cluster) set.seed(109) # Initialize total within sum of squares error: wss wss \u0026lt;- 0 # For 1 to 30 cluster centers for (j in 1:30) { km.out \u0026lt;- kmeans(scaled_data, centers = j, nstart = 20) # Save total within sum of squares to wss variable wss[j] \u0026lt;- km.out$tot.withinss } # create a DF to use in a ggplot visualisation wss_df \u0026lt;- data.frame(num_cluster = 1:30, wgss = wss) # plot to determine optimal k ggplot(data = wss_df, aes(x=num_cluster, y= wgss)) + geom_line(color = \u0026quot;lightgrey\u0026quot;, size = 2) + geom_point(color = \u0026quot;green\u0026quot;, size = 4) + theme_fivethirtyeight() + geom_curve(x=14, xend=8, y=300000, yend= 290500, arrow = arrow(length = unit(0.2,\u0026quot;cm\u0026quot;)), size =1, colour = \u0026quot;purple\u0026quot;) + geom_text(label = \u0026quot;k = 8\\noptimal level\u0026quot;, x=14, y= 290000, colour = \u0026quot;purple\u0026quot;) + labs(title = \u0026quot;Using Eight Clusters To Group Players\u0026quot;, subtitle = \u0026quot;Selecting the point where the elbow \u0026#39;bends\u0026#39;, or where the slope of \\nthe Within groups sum of squares levels out\u0026quot;) # Set k equal to the number of clusters corresponding to the elbow location k \u0026lt;- 8 # Create a k-means model on wisc.data: wisc.km wisc.km \u0026lt;- kmeans(scale(data_cluster), centers = k, nstart = 20) # add the cluster group back to the original DF for all players other than GK and Unknown cluster_data \u0026lt;- fifa_data %\u0026gt;% filter(PositionGroup != \u0026quot;Unknown\u0026quot;) %\u0026gt;% filter(PositionGroup != \u0026quot;GK\u0026quot;) %\u0026gt;% mutate(Cluster = wisc.km$cluster) K-means has split the data into the following 8 clusters as outlined below.\nIt appears that clusters 4 and 8 are representing the more defensive-midfielder types (made up mainly of DEFs and MIDs). Cluster 7 looks to be grouping very attacking minded midfielders midfielders (mande up mainly of MIDs and FWDs).\nCluster 2 is grouping Midfielders, Cluster 5 is for Forwards, while clusters 1 and 3 are for the Defenders.\n## ## DEF FWD MID ## 1 1137 43 1802 ## 2 1498 39 1282 ## 3 1443 7 136 ## 4 16 742 1668 ## 5 20 989 1001 ## 6 143 269 614 ## 7 1608 3 185 ## 8 1 1326 150 Plotting the 20 highest rated players in each cluster shows us which players are similar in terms of their skill sets.\nI am going to build a shiny app based off this clustering analysis to enable users to input a player they wish to find a similar replacement for, and the resulting output displaying a list of players and their attributes who are most similar.\nFor example, selecting I. Perišić from Inter as someone to replicate, the below results will appear.\nThe function will take three inputs, the player name how many players you want returned, and what how close to the player’s value do you want to see results for (ie if you want all players within 10% of the player’s value, use 0.1).\nreturn_similar_players \u0026lt;- function(player, num_results, return_within_fraction) { cluster_filter \u0026lt;- cluster_analysis$Cluster[cluster_analysis$Name == player] player_value \u0026lt;- cluster_analysis$ValueNumeric_pounds[cluster_analysis$Name == player] cluster_analysis %\u0026gt;% filter(Cluster == cluster_filter, ValueNumeric_pounds \u0026gt;= (player_value * (1- return_within_fraction)) \u0026amp; ValueNumeric_pounds \u0026lt;= (player_value * (1 + return_within_fraction))) %\u0026gt;% head(num_results) } return_similar_players(\u0026quot;I. Perišić\u0026quot;, 100, .05) %\u0026gt;% kable(format = \u0026quot;html\u0026quot;, escape = F) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;, full_width = F)   ID  Name  Club  Age  PositionGroup  Overall  Cluster  ValueNumeric_pounds      189332  Jordi Alba  FC Barcelona  29  DEF  87  6  38000000    191043  Alex Sandro  Juventus  27  DEF  86  6  36500000    184087  T. Alderweireld  Tottenham Hotspur  29  DEF  86  6  39000000    197445  D. Alaba  FC Bayern München  26  DEF  85  6  38000000    189513  Parejo  Valencia CF  29  MID  85  6  37000000    187961  Paulinho  Guangzhou Evergrande Taobao FC  29  MID  85  6  37000000    184941  A. Sánchez  Manchester United  29  FWD  85  6  37500000    184267  Y. Brahimi  FC Porto  28  MID  85  6  39000000    181458  I. Perišić  Inter  29  MID  85  6  37500000    179844  Diego Costa  Atlético Madrid  29  FWD  85  6  38500000    165153  K. Benzema  Real Madrid  30  FWD  85  6  37000000    205498  Jorginho  Chelsea  26  MID  84  6  38000000    204970  F. Thauvin  Olympique de Marseille  25  MID  84  6  39000000    200104  H. Son  Tottenham Hotspur  25  MID  84  6  37000000    212523  Anderson Talisca  Guangzhou Evergrande Taobao FC  24  MID  83  6  36500000     This code will be able to be re-used on FIFA 20 data when it comes out - feel free to use this as a basis for any further analysis!\nWould love your thoughts on the analysis.\n ","date":1566000000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566000000,"objectID":"34f51ac4a555785eeeaaeb1838035011","permalink":"/blog/an-in-depth-analysis-of-fifa19/","publishdate":"2019-08-17T00:00:00Z","relpermalink":"/blog/an-in-depth-analysis-of-fifa19/","section":"post","summary":"This post was originally released on the Kaggle FIFA 19 Complete Player dataset that was kindly collected by Karan Gadiya. Many thanks Karan. The link to my original kernel is here","tags":["R","tidyverse","ggplot","fifa","fifa19","analytics","data analysis"],"title":"An In Depth Analysis of FIFA 19","type":"post"},{"authors":null,"categories":["R","sport","afl","analytics","tidyverse"],"content":"  Jarryd Roughead - four-time Hawthorn premiership player and all round great man - formally announced his retirement this week, with his last game coming over the weekend against the Gold Coast Suns.\nNot only was he a pivotal four-time premiership star for the Hawthorn Hawks, he survived a hell of a lot of adversity with some very serious cancer scares and handled himself with nothing but grace and dignity. He came back to captain his club in 2017 and while the on-field element was tough to come back to, he was lumped with the burden of captaining the club after they parted ways with stalwarts Sam Mitchell and Jordan Lewis after the 2016 season.\nHe filled key posts down back during the early part of his career, then moved up forward where he played a very, very good second option, then also carved out a valuable ruck/midfielder role for the club a bit later on. He could play everywhere.\nI loved him as a footballer, but the man he seems to be makes me envious I don’t know him personally. I wanted to take an in depth (but different) look at the career he had with the mighty Hawks using some frequently used data visualisations.\nA lot of the articles written about this great man will focus on the more well known numbers of his career… the 282 (to be 283 after his last game), the 572 goals, the four flags, Coleman Medal in 2013 and All Australian selections in 2013-14. Rightfully so too.\nThis post however is going to look at some of the more obscure aspects of Roughy’s playing time so we have a full view of his time in the game.\nThe data for this post originally comes from AFL Tables, but through Jimmy Day’s amazing fitzRoy R package.\nlibrary(fitzRoy) library(stringr) library(tidyverse) library(lubridate) theme_set(theme_minimal()) stats_players \u0026lt;- get_afltables_stats(start_date = \u0026quot;2000-01-01\u0026quot;, end_date = \u0026quot;2019-10-01\u0026quot;) rough \u0026lt;- stats_players %\u0026gt;% filter(str_detect(First.name, \u0026quot;Jarryd\u0026quot;), str_detect(Surname, \u0026quot;Roughead\u0026quot;)) rough \u0026lt;- rough %\u0026gt;% mutate(final = ifelse(str_detect(Round, \u0026quot;F\u0026quot;), \u0026quot;Y\u0026quot;, \u0026quot;N\u0026quot;), Opponent = ifelse(Home.team == Playing.for, Away.team, Home.team)) rough \u0026lt;- rough %\u0026gt;% mutate(Date = ymd(Date), Weekday = wday(Date, label = T), StartingHour = substr(Local.start.time, 1,2 ), TimePeriod = ifelse(as.numeric(StartingHour) \u0026lt; 17, \u0026quot;Afternoon\u0026quot;, \u0026quot;Evening\u0026quot;)) When were Roughy’s games played? Of Roughy’s 282 games, not one of them was played on a Wednesday - understandable given the Hawks don’t have any ANZAC Day games and no other holiday falls on a Wednesday. Every other day of the week he played. Saturdays are where he played most of his games, with 47% of his games played then.\nThe most frequent starting time of games Roughy played was in the 20th hour of the day (start time between 7pm and 8pm), followed by games starting between 2pm to 3pm.\n Where were Rough’s games played? No surprises to see that Roughy played most of his games at either the ’G or in Tasmania.\nInterestingly, the only interstate finals Rough ever played in were at Subiaco (in Western Australia), where he played three (two of those came in the same finals series - the 2015 season, where the Hawks won their third premiership in a row).\n What conditions did the great man play in? For this section, I will use the weather data from my series on AFL crowds and the data and code for collecting the data can be found here.\n# read in data rain_data \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/JaseZiv/AFL-Crowd-Analytics/master/data/cleaned_data/afl_preprocessed.csv\u0026quot;, stringsAsFactors = F) # select just the required variables, first creating a key to join back to the \u0026#39;rough\u0026#39; df rain_data \u0026lt;- rain_data %\u0026gt;% select(team1, team2, date, rainfall_clean, min_temp, max_temp) %\u0026gt;% mutate(gameId = paste(team1, team2, date, sep = \u0026quot;-\u0026quot;)) %\u0026gt;% select(-team1, -team2, -date) # join data back rough \u0026lt;- rough %\u0026gt;% mutate(gameId = paste(Home.team, Away.team, Date, sep = \u0026quot;-\u0026quot;)) %\u0026gt;% left_join(rain_data, by = \u0026quot;gameId\u0026quot;) %\u0026gt;% select(-gameId) The wettest game Roughy played in was a 2012 loss against Richmond at the MCG, where 28ml fell in 24 hours. In a dirty day for the Hawks, Roughy had an ok game, kicking 2 goals 1 behind and took 6 marks.\nThe coldest game he played in (the snow game last week in Canberra would’ve won had he played) was in Tasmania - no surprises - against the Lions in that horrible premiership hangover year in 2009. The maximum temperature was a crisp 10.8 degrees and there was even 5.2ml of rain. Roughy had a game to forget, with 7 kicks and only 1 goal. A stinker for the whole club.\nOn the flip side, the round three matchup in Rough’s first season against Essendon was the hottest game he played in… again the Hawks lost this one in a tight two-point loss. Rough managed to kick two goals.\n What about Chas Brownlow? Not known for his prolific Brownlow vote getting, the big Rough received 59 votes in total, at an average of 0.225 votes per game.\nRoughy’s All Australian years in 2013 and 2014 saw his average votes per games peak; he received 0.59 and 0.55 votes per game respectively. Five of his 14 seasons saw zero votes awarded.\n Who umpired the big man? Roughy has seen 99 umpires during his time in the game.\nBrett Rosebury officiated 41 of Roughy’s games, with Matt Nicholls and Matt Stevic following closely behind with 37 times each. Twelve umpires officiated Roughy’s games on at least 20 occasions\nOf the 41 games Rosebury officiated, 13 were in finals - by far the most finals games with Roughy than any umpire. Of the umpires with at least 20 games, only Stephen McBurney hasn’t umpired Rough in a final, while Troy Pannell has umpired him once.\nWere some kinder to Rough? We can answer this question looking at Brownlow votes and Free Kick counts… The analysis will limit itself to those 12 umpires who have officiated his games at least 20 times.\nI always knew I disliked Razor Ray… now I know why. He and Michael Vozzo were the only umpires on this list to fail to award a Brownlow vote to the big man. Shaun Ryan awarded them at a rate of just over half a vote for the 23 games he officiated Roughy’s games.\nNot only did Shaun Ryan award Brownlow votes at a higher rate than any of his brethren on this list, he also didn’t discriminate between how many frees were awarded for and against the big man, though he did award 1.05 frees against, the fifth highest amount on average.\nMichael Vozzo had the largest discrepancy between the number of frees, awarding less than 0.5 frees for Roughy (0.478), while penalising the big Rough at a rate of 1.09 per game.\nAt 1.1 frees against per game, the pest Razor and Shane McInerney penalised Rough the most, while Simon Meredith (0.75) penalised him the least (0.84).\nTroy Pannell awarded Roughy the most frees on average (1.29).\n  Finally, Bouncing Not only was Roughy a superstar footballer, he was a gun junior basketballer.\nBouncing is an important piece of the basketball puzzle… I wonder if Rough bounced the ball much?\nThe average number of bounces per player per game is 0.529. Throughout Roughy’s career, he bounced the ball at a rate of 0.12 per game… a fair bit off the average…\nHis rookie season was his best bouncing season at a rate of 0.562 bounces per game.. Clarko must have ironed that out of his game and must have been thrilled in 2009 (other than the premiership hangover) when Roughy didn’t bounce the ball once.\nWell that’s all for this tribute to the great Jarryd Roughead.\nLooking forward to watching his last game, and then seeing him around the media hopefully.\nWe love ya Rough!\nAs always, feel free to get in touch if there’s any feedback here\n ","date":1565740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565740800,"objectID":"16b43575f5e57332fdf6247dc8d1cf53","permalink":"/blog/a-tribute-to-jarryd-roughead-using-data-analysis/","publishdate":"2019-08-14T00:00:00Z","relpermalink":"/blog/a-tribute-to-jarryd-roughead-using-data-analysis/","section":"post","summary":"Jarryd Roughead - four-time Hawthorn premiership player and all round great man - formally announced his retirement this week, with his last game coming over the weekend against the Gold Coast Suns.","tags":["R","tidyverse","ggplot","jarryd roughead","afl"],"title":"A tribute to Jarryd Roughead with some obscure data","type":"post"},{"authors":null,"categories":["R","sport","afl","analytics","tidyverse","machine learning","linear regression"],"content":"  Introduction There is a lot of talk about crowd behaviour and crowd issues with the modern day AFL. I personally feel the crowd issues are a case of business-as-usual; I’ve been going to AFL games on-and-off for close to three decades and the crowd behaviour is no different today as it was 20 years ago… just now everyone with a phone and Twitter account is a journo.\nAnyway, that’s my two cents worth. This analysis is not on crowd behaviour. This post is primarily concerned with building a model to predict AFL crowds during the season proper. While data was collected since the beginning of the 2000 season, the model will be bult on training data from the beginning of the 2013 season to the end of the 2018 season (Finals series excluded), with the trained model then used to predict the attendance figures of AFL games during the 2019 season up until round 18 which concluded on Sunday 21st July, 2019. The linear model will be built using R (version 3.5.3) in RStudio.\nThis post is a follow up to my introductory analysis done on bandwagon fans. It can be found here.\nThe article was inspired by the amazing work Tony Corke does, specifically his post on The Predictability of AFL Crowds.\nAll code for this project can be found on GitHub here\nThe Data Three sources of data were used in the project:\nAFL Tables was used as the primary source of information. This is no doubt the go to for anyone looking at doing AFL analysis Australian Sports Betting was used to get betting data for matches from the 2013 season Australian Bureau of Meteorology was used for climat data (rain and temperature). Part of the scraper for BOM data was taken from James Day’s fitzRoy R package. Big thanks to him. The rain and temperature data had some missing observations. Missing rain data was filled by averaging out the next taken reading over the missed data points before it. Missing temperature was filled taking the average temperature for that month.    Feature Engineering The data contained at AFL Tables is a rich source of information, but some steps were needed to glean some additional information from the data.\nThe original three data sources combined contained approximately 35 variables for potential use. Feature engineering (the process of creating additional explanatory variables to use in a predictive model) was then conducted and resulted in ~95 features in total to see if they would ultimately be added to the final model to be used.\nFeatures Created Based on some feedback from the Banwagon article and other things that I wanted to explore, some new features were created to see if they had any predictive power.\nTake the game start time as an example - having the time is great, but a regression model may find it better to categorise the time to a period of the day to reduce some of the noise. So, with that in mind, a feature (time_period) was created - if the game started by 5pm, I have classified it as an afternoon game. Games started between 5pm-7pm were classified as evening games, while games later than this were night games.\nAdditionally, the day of the week and the time_period were joined into one variable game_time to use in the model. For example, if it was a Saturday game starting at 5:30pm, it was classified as “Sat-Evening”.\nVenues that were used infrequently, or more accurately, weren’t the teams primary “home” stadium were labelled as “Other”. There were 94 games played at these venues, and this includes at stadiums like Eureka Stadium, Cazaly’s Stadium, Traeger Park, and international venues Wellington in NZ and Jiangwan Stadium in China.\nAnother feature was created to determine rivalry games - the thinking being that when two fierce rivals face off against each other, the crowds will come. To be a rivalry game, a few different options existed. The first contains the four oldest traditional rivals, all from Melbourne - Collingwood, Richmond, Essendon and Carlton (hold your Carlton opinions, I know they’ve been horrible but their fans still turn up to these rivalry games). The Derby in SA (The Showdown between Adelaide and Port Adelaide) and in WA (Western Derby between West Coast and Fremantle) are also classed as rivalry games.\nAfter hearing a key AFL staffer talk a few years back talk about betting markets being a strong predictor of attendances, I collected betting data to test this assumption. A number of features were created in addition to the raw odds and line data scraped. These included calculating the differences between the odds of the home and away team and also calculating the change in betting throughout the week.\nFeatures were also created of the average experience of both the home and away teams, as were milestone games (where players of either team are playing in their 100, 200, 250 or 300th games).\nEach team’s last game result was calculated, as was the number of wins each team had in their previous three games. Additionally, whether the team played finals the previous season was identified.\nA ladder at each round was created and ladder position, percentage and score for and against was explored as possible features for the end model.\nA feature was also created to identify games played by teams from the same state or not.\n  Exploratory Data Analysis Before we start building the model, we need to understand both the response and predictor variables. The most effective way to do this is through visualisations, and what package is better to use than ggplot2!\nThis section will contain some Exploratory Data Analysis (more commonly known as ‘EDA’), which is a natural first step in any data analysis project. It is an extrememly important step and allows the analyst to inspect the data - what shape does the target and predictor variables have, are there any correlations between the variables that may be included in a predictive model, are there any outliers in the data, or any missing data… you get the point, EDA is very important!\nWell there is no missing data in the dataset being used… that has all been cleaned in the pre-processing stage that can be found in this R code.\nThe Response Variable - Attendance Let’s look at our response variable - or the thing we’re trying to predict - attendance at AFL premiership season games.\nWe can see that the attendance figures are somewhat positively skewed. We’ll see if the few higher attendance games cause any issues later on. This variable may need to undergo some form of transformation, but more on that later.\nThe median attendance to AFL games during the regular season is just under 32,500 for the 1,340 games played since 2013.\n Some Key Predictors data_for_model \u0026lt;- afl_premiership_season %\u0026gt;% select(season, attendance, team1, team2, round, venue, game_time, home_median_experience, home_mean_experience, away_median_experience, away_mean_experience, home_300, home_milestone, away_300, away_milestone, count_milestones, rainfall_clean, min_temp, max_temp, last_five_days_rain, temperature_diff, HomeOddsOpen, AwayOddsOpen, HomeLineOpen, AwayLineOpen, odds_diff, rivalry_game, HomeTeamFav, team1_last_result, team2_last_result, split_round, home_team_finals_last_season, away_team_finals_last_season, season_stage, temperature, IsHomeMilestone, IsAwayMilestone, count_milestones, last_results, home_team_state, away_team_state, is_same_state, temperature_diff, home_odds_change, away_odds_change, home_line_change, away_line_change, second_home_game, home_season_points, home_score_for, home_percentage, home_ladder_pos, away_wins_last_three, away_season_points, away_score_for, away_percentage, away_ladder_pos, teams_in_eight, finals_last_season, home_wins_last_three, away_wins_last_three) %\u0026gt;% na.omit() As was evident in the first post of this series, there was noticeable variation in attendances between the teams, and also whether teams won or lost their previous game. Finally, the first post showed that the home team’s favouritism status also seemed to have a relationship with attendance.\nCategorical Features:\nSome of the key categorical variables created that we would expect to have an impact on crowd figures are displayed below.\nSome observations:\n There appears to be a relationship between the venue the game is played at and the attendance. Stadiums classified as Other include secondary home grounds and some of the ovals not used in 2019. There were 238 of the 1,340 games at these stadiums\n Whether either the home or away team played finals last year seems to have a weak relationship with attendance. It may be useful to include in our model\n When the game is played and attendance appear to be related. Weekday Afternoon and Friday Afternoon are high because of ANZAC day clashes between Essendon and Collingwood no doubt. Friday and Sunday night games seem to draw better crowds. Could be a useful predictor\n There appears to be some variance in attendance figures between the different rounds during a season. Could be a useful predictor\n Games classed as “Rivalry Games” clearly draw larger attendances than non-rivalry games. Could be a useful predictor\n Whether the round was a split round (where some teams have a bye, or week off) or not doesn’t appear to be related to attendances. Might not be a useful predictor\n There appears to be slightly higher crowds when the home team has a player playing a milestone (either a 100, 200, 250 or 300th game), but the relationship certainly doesn’t appear strong. Might not be a useful predictor\n When the two competing teams are from the same state, crowds appear to be higher. Could be a useful predictor\n To be expected, games where the home team is playing the game at their second home ground (Hawthorn and North in Tasmania, Melbourne, Western Bulldogs in Darwin, etc) draw lower crowds. This is probably more to do with lower statium capacities, but may still be a useful feature\n  Numerical Features:\nTo explore the relationship between the numerical features and attendance, Pearson’s correlation will be employed.\nUsing dplyr to select just the numerical features and then calculate the correlations on the converted matrix yields the below correlations with attendance.\nnumeric_features \u0026lt;- data_for_model %\u0026gt;% select_if(is.numeric) numeric_features %\u0026gt;% as.matrix() %\u0026gt;% cor() %\u0026gt;% .[,\u0026quot;attendance\u0026quot;] %\u0026gt;% sort(decreasing = T) ## attendance home_mean_experience home_percentage ## 1.00000000 0.26242775 0.23603629 ## home_median_experience home_wins_last_three away_mean_experience ## 0.23224717 0.17740024 0.16024146 ## away_median_experience home_season_points away_percentage ## 0.12698259 0.10829187 0.10549367 ## away_wins_last_three season AwayLineOpen ## 0.08874187 0.06906937 0.06717077 ## temperature_diff count_milestones home_line_change ## 0.04693253 0.04502199 0.03176978 ## away_milestone home_300 home_milestone ## 0.02947022 0.02805048 0.02612771 ## away_season_points away_300 home_odds_change ## 0.01964212 0.01305329 0.01234577 ## away_odds_change home_score_for away_line_change ## -0.01732249 -0.02851372 -0.03176978 ## AwayOddsOpen rainfall_clean away_score_for ## -0.05413613 -0.06190488 -0.06313146 ## HomeLineOpen last_five_days_rain round ## -0.06717077 -0.08122135 -0.08681637 ## max_temp min_temp away_ladder_pos ## -0.09200801 -0.13811543 -0.15435393 ## HomeOddsOpen odds_diff temperature ## -0.15999933 -0.16722767 -0.20128979 ## home_ladder_pos ## -0.27541679 While all correlations appear to be fairly weak, the following observations can be made:\n The average experience (home_mean_experience) for the home team, calculated as the average games played by each player at each game they play in, yields the highest Pearson correlation with attendance of 0.264\n The Home team’s percentage (scores for / scores against) also have a correlation above 0.2 at 0.236\n The number of wins the home team had in their last three games had a Pearson correlation of 0.175, while the correlation of away team’s average playing experience was 0.159\n Strangely, the home team’s ladder position had a negative correlation, with a Pearson correlation of -0.273. The away team’s ladder position also had a negative correlation of -0.150\n Betting data shows some relationships; as the HomeOddsOpen increase, the attendance tends to decrease, which is somewhat expected. The odds_diff variable (the difference between the home teams opening odds and the away teams) also has a weak negative relationship (correlation -0.168)\n Weather data shows some relationships. As expected, the maximum temperation and rainfall, both on the day and the accumulation of last five day’s rainfall has a negative relationship with attendance (-0.091, -0.081 and -0.65 respectively)\n    Building the model In any project that aims to build a machine learning model to predict the outcome of some response variable, it’s important to hold out a subset of your data when training the model. These two sets are generally referred to as training and test sets. The training set is what you pass to the model to build, and then the resulting predictions are made on the data the model hasn’t “seen”, to best mimic real life examples.\nIn this case, the model will be trained on data from the 2013 to 2018 seasons inclusive (afl_pre_2019), with the predictions then made on 2019 data that the model hasn’t seen, afl_2019. This is done to ensure the model doesn’t “overfit” the data. Models that overfit the data generally perform worse on unseen accurrences.\nThings that were attempted but not included in the model The following features were thought to have some validity but when run through the model were found to have no predictive power:\n The number of wins the teams had in their last three games didn’t contribute The rainfall on the day had no predictive power (however the total rainfall for the five days leading up to game day did) Milestone games made the model perform worse   What’s missing / would be good to have There is some information that wasn’t available publicly (or I didn’t try to get) which may prove handy in any future modelling work. These include:\n Ticket pricing for games during each season to explore the effect ticket pricing has on attendance Whether tickets were given away to fans for games and whether that had an impact on attendances Any social media posts / newspaper text analysis regarding teams / topical matters involving players playing in games, etc  afl_pre_2019 \u0026lt;- data_for_model %\u0026gt;% filter(season \u0026lt; 2019, season \u0026gt;= 2013) afl_2019 \u0026lt;- data_for_model %\u0026gt;% filter(season == 2019)  Baseline Model Before building a model to predict games, we want a baseline model to be able to compare to. The temptation would be there to use the average crowd for each game as a baseline, but the different venue capacities make this less than relevant. As a result, to calculate the baseline model, I will use the median attendance for each vanue (taking the average of the venues labelled “Other” as a whole), for each home-away team combo. For example, Collingwood(H) vs Essendon(A) is different to Essendon(H) vs Collingwood(A). The median is used because of the slightly skewed nature of AFL crowds.\nmedian_attendances \u0026lt;- afl_pre_2019 %\u0026gt;% group_by(team1, team2, venue) %\u0026gt;% summarise(baseline_attendance = median(attendance)) %\u0026gt;% ungroup() baseline \u0026lt;- afl_2019 %\u0026gt;% select(team1, team2, venue, attendance) %\u0026gt;% left_join(median_attendances, by = c(\u0026quot;team1\u0026quot;, \u0026quot;team2\u0026quot;, \u0026quot;venue\u0026quot;)) %\u0026gt;% filter(!is.na(baseline_attendance)) Using the average attendances per ground as our baseline, an RMSE of 6,816 and a mean absolute error of 4,958 is achieved. Any model that is built from here needs to be better than this.\n Transforming Response Variable Because of the somewhat skewed nature of our response variable, it might be wise to undergo some sort of transformation to normalise it more. A log transformation is routinely used to “unskew” data. Below is the result of log transforming the attendance variable.\nIt looks less skewed than the non-transformed variable, however (spoiler alert), the models performed worse with attendance being log transformed.\n  Feature Selection Using Stepwise Method A popular method to select which features go into a model is using stepwise regression. This can either be performed:\n forward, where variables are added one by one, with only those that improve the model’s explainability (R-squared) retained, backward, which starts with all variables, then removes them if they don’t add value to the model’s explainability, or both, which is an ensemble of both forward and backward feature selection.  Using stepwise in both directions maximised the model’s Adjusted R-squared measure at 0.87.\nWhen performing linear regression, we want to ensure there is no collinearity between the variables. One way to asses this is to calculate the Variable Inflation Factor (VIF). This is made easy with the vif function in the car package. A VIF above 5 is considered to be too high.\nWhile the round the game is played is just over 5 (5.7), we will keep this in, however odds_diff and home_score_for will be removed from the full model selected using stepwise and re-run.\nfull_model \u0026lt;- lm(attendance ~ ., data = afl_pre_2019) # Set seed for reproducibility set.seed(123) step.model \u0026lt;- MASS::stepAIC(full_model, direction = \u0026quot;both\u0026quot;, trace = FALSE) car::vif(step.model) ## GVIF Df GVIF^(1/(2*Df)) ## team1 3.760325e+07 17 1.670324 ## team2 8.911787e+00 17 1.066449 ## round 3.249472e+01 1 5.700414 ## venue 1.490291e+07 9 2.503312 ## game_time 3.502667e+00 11 1.058633 ## home_milestone 1.077704e+00 1 1.038125 ## rainfall_clean 1.600109e+00 1 1.264954 ## min_temp 2.442144e+00 1 1.562736 ## max_temp 2.943632e+00 1 1.715702 ## last_five_days_rain 1.624607e+00 1 1.274601 ## HomeOddsOpen 7.850335e+01 1 8.860212 ## AwayOddsOpen 1.469414e+02 1 12.121938 ## HomeLineOpen 1.017707e+01 1 3.190152 ## odds_diff 1.645084e+02 1 12.826085 ## rivalry_game 1.682047e+00 1 1.296937 ## split_round 1.097575e+00 1 1.047652 ## last_results 1.901405e+00 3 1.113044 ## is_same_state 2.561134e+00 1 1.600354 ## home_season_points 1.171072e+01 1 3.422093 ## home_score_for 4.401100e+01 1 6.634079 ## away_season_points 1.013471e+01 1 3.183506 ## away_ladder_pos 8.299211e+00 1 2.880835 ## teams_in_eight 1.384744e+01 3 1.549631 ## home_team_finals_last_season 1.855485e+00 1 1.362162 # the backward selection model include a variable with hi collinearity (VIF = 12). Will recreate model without odds_diff full_model_clean \u0026lt;- lm(attendance ~ team1 + team2 + round + venue + game_time + home_milestone + rainfall_clean + min_temp + max_temp + last_five_days_rain + HomeOddsOpen + AwayOddsOpen + HomeLineOpen + rivalry_game + split_round + last_results + is_same_state + home_season_points + away_season_points + away_ladder_pos + teams_in_eight + home_team_finals_last_season, data = afl_pre_2019) # summary(full_model_clean) car::vif(full_model_clean) ## GVIF Df GVIF^(1/(2*Df)) ## team1 3.221921e+07 17 1.662750 ## team2 8.429548e+00 17 1.064706 ## round 1.151616e+01 1 3.393547 ## venue 1.470969e+07 9 2.501498 ## game_time 3.423992e+00 11 1.057540 ## home_milestone 1.074063e+00 1 1.036370 ## rainfall_clean 1.588738e+00 1 1.260451 ## min_temp 2.440252e+00 1 1.562131 ## max_temp 2.939080e+00 1 1.714374 ## last_five_days_rain 1.623319e+00 1 1.274095 ## HomeOddsOpen 2.900135e+00 1 1.702978 ## AwayOddsOpen 3.400223e+00 1 1.843969 ## HomeLineOpen 9.825720e+00 1 3.134600 ## rivalry_game 1.680106e+00 1 1.296189 ## split_round 1.096713e+00 1 1.047241 ## last_results 1.885393e+00 3 1.111477 ## is_same_state 2.559836e+00 1 1.599949 ## home_season_points 7.899789e+00 1 2.810656 ## away_season_points 9.963380e+00 1 3.156482 ## away_ladder_pos 8.290049e+00 1 2.879245 ## teams_in_eight 1.339704e+01 3 1.541114 ## home_team_finals_last_season 1.848796e+00 1 1.359704 The cleaned model’s Adjusted R-squared is 0.8652, while the RMSE was 6,474 and MAE 4,855. These results better the baseline model, but only just.\n Best model: Determining the “best” model is an interesting exercise. If the explainability powers of the model is what we’re after, then we want to maximise the Adjusted R-squared metric. This analysis is aimed at maximising the success of predicting crowds, and as such, the Root Mean Squared Error (RMSE) or Mean Absolute Error (MAE) are the metrics we’re going to focus on. Both measures can be related back to the response variable - attendance - the difference being that MAE is the average absolute error of the model (difference between prediction and actual), while the RMSE penalises the model the larger the error.\nFor both RMSE and MAE, the below model results in the lowest RMSE and MAE when applied to the 2019 season games.\nset.seed(123) fit_lm \u0026lt;- lm(attendance ~ team1 + team2 + round + venue + game_time + I(HomeLineOpen * home_ladder_pos) + rivalry_game + last_results + last_five_days_rain + max_temp + min_temp + home_mean_experience + away_mean_experience + is_same_state + home_score_for + I(home_percentage * home_ladder_pos) + I(home_wins_last_three * round) + round * finals_last_season, data = afl_pre_2019) summary(fit_lm)$adj.r.squared ## [1] 0.8582593 The Adjusted R-Squared of the model is 0.858, indicating that almost 86% of the variablility in AFL attendances in the training set (2013-2018) is explained by the model.\nInteraction Terms\nFeatures were not only used on their own in this model. Interaction terms wera also employed. The following features were combined to form a feature and gave the model a boost in predictive power:\n I(HomeLineOpen * home_ladder_pos) I(home_percentage * home_ladder_pos) I(home_wins_last_three * round) round * finals_last_season  Analysing the best model The first thing we want to do is ensure the residuals are normally distributed to satisfy the condition of normality for linear models.\nThe below histogram suggests that the errors are fairly normally distributed and centred around zero. A good sign.\nggplot(data = data.frame(fit_lm$residuals), aes(x= fit_lm.residuals)) + geom_histogram() + ggtitle(\u0026quot;ERRORS ARE CENTRED AROUND ZERO\u0026quot;) + scale_x_continuous(labels = comma, name = \u0026quot;Model Residuals\u0026quot;) # ggsave(\u0026quot;plots/prediction_plots/errors_hist.png\u0026quot;, width = 30, height = 22, units = \u0026quot;cm\u0026quot;) Then we can look at the coefficients of each of the predictor variables.\nTo make inspecting these easier, the broom package was used. Using the tidy() function, the results of the summary lm output are nicely displayed in a DF.\nThe coefficients for each of the predictor variables (other than home and away teams) are displayed below.\na \u0026lt;- broom::tidy(fit_lm) %\u0026gt;% mutate(p.value = round(p.value, 5)) a %\u0026gt;% filter(!str_detect(term, \u0026quot;team1\u0026quot;), !str_detect(term, \u0026quot;team2\u0026quot;)) %\u0026gt;% kableExtra::kable(format = \u0026quot;html\u0026quot;, escape = F) %\u0026gt;% kableExtra::kable_styling(\u0026quot;striped\u0026quot;, full_width = F)    term  estimate  std.error  statistic  p.value      (Intercept)  61007.772536  7066.622895  8.6332288  0.00000    round  -628.797448  147.199874  -4.2717255  0.00002    venueCarrara  -9384.433835  4893.988254  -1.9175432  0.05543    venueDocklands  2025.790428  1843.347042  1.0989740  0.27202    venueGabba  -12078.813180  7915.867406  -1.5258989  0.12732    venueKardinia Park  -790.290125  2477.093571  -0.3190393  0.74976    venueM.C.G.  14604.069479  1859.992224  7.8516831  0.00000    venueOther  -11316.633901  1521.161777  -7.4394677  0.00000    venuePerth Stadium  2344.181828  2119.734512  1.1058846  0.26902    venueS.C.G.  -6985.844583  2854.176355  -2.4475869  0.01454    venueSydney Showground  -13915.330942  2344.860834  -5.9343952  0.00000    game_timeFri Evening  -14669.992661  7858.171556  -1.8668456  0.06219    game_timeFri Night  -26232.422896  6419.398624  -4.0864300  0.00005    game_timeSat Afternoon  -27517.789656  6413.609760  -4.2905307  0.00002    game_timeSat Evening  -28587.419367  6425.144735  -4.4493036  0.00001    game_timeSat Night  -27304.481145  6416.346545  -4.2554561  0.00002    game_timeSun Afternoon  -28302.693645  6410.217064  -4.4152473  0.00001    game_timeSun Evening  -30499.992506  6441.388865  -4.7350025  0.00000    game_timeSun Night  -29709.469650  6969.788830  -4.2626069  0.00002    game_timeWeekday Afternoon  -6264.436854  6587.953778  -0.9508927  0.34187    game_timeWeekday Evening  -32666.780662  8987.962304  -3.6345035  0.00029    game_timeWeekday Night  -24296.860101  6490.825702  -3.7432618  0.00019    I(HomeLineOpen * home_ladder_pos)  -3.929212  0.897336  -4.3787523  0.00001    rivalry_gameRivalry  4243.471176  1103.664170  3.8448935  0.00013    last_resultsboth_won  3278.290173  599.102681  5.4720005  0.00000    last_resultsonly_away_team_won  1354.077316  536.942587  2.5218289  0.01181    last_resultsonly_home_team_won  1381.030990  594.706552  2.3222058  0.02040    last_five_days_rain  -44.287937  12.297250  -3.6014505  0.00033    max_temp  310.452321  61.108889  5.0803137  0.00000    min_temp  -125.802409  59.279520  -2.1221901  0.03404    home_mean_experience  25.443372  14.678437  1.7333843  0.08331    away_mean_experience  45.390519  14.339468  3.1654256  0.00159    is_same_stateyes  8151.587887  601.638591  13.5489778  0.00000    home_score_for  7.394136  1.692602  4.3685025  0.00001    I(home_percentage * home_ladder_pos)  -294.795766  66.179029  -4.4545193  0.00001    I(home_wins_last_three * round)  3.645874  19.738164  0.1847119  0.85349    finals_last_seasonboth_played  1691.406848  1188.541482  1.4230945  0.15499    finals_last_seasonhome_only_played  953.657176  1235.936253  0.7716071  0.44051    finals_last_seasonneither_played  482.072356  1101.460684  0.4376664  0.66171    round:finals_last_seasonboth_played  17.403974  83.889701  0.2074626  0.83569    round:finals_last_seasonhome_only_played  -13.710531  82.253765  -0.1666858  0.86765    round:finals_last_seasonneither_played  21.455146  76.032301  0.2821846  0.77785     actual_2019crowds \u0026lt;- afl_2019$attendance # fit linear model afl_2019$predicted_attendance \u0026lt;- predict(fit_lm, afl_2019) # put a floor on AFL attendances - surely the AFL doesn\u0026#39;t let the crowd fall below 6,000 afl_2019$predicted_attendance \u0026lt;- ifelse(afl_2019$predicted_attendance \u0026lt; 6000, 6000, afl_2019$predicted_attendance) # calculate the errors error \u0026lt;- afl_2019$predicted_attendance - afl_2019$attendance Then we want to annalyse how well the model fit the relationship. This can be achieved by plotting the actual vs predicted values on a scatterplot.\nI can be seen that the linear model does a fairly good job on average. There model did under-predict some of the larger drawing games, while also predicting an attendance of over 30,000 to the Hawthorn vs GWS game at the MCG in round 8, which only drew 14,636 fans.\nafl_2019 %\u0026gt;% ggplot(aes(x=attendance, y= predicted_attendance)) + geom_point() + geom_abline(slope = 1, intercept=0) + ggtitle(\u0026quot;MODEL UNDER-PREDICTED SOME\\nLARGE DRAWING GAMES\u0026quot;) + scale_x_continuous(labels = comma, name = \u0026quot;Actual Attendance\u0026quot;) + scale_y_continuous(labels = comma, name = \u0026quot;Predicted Attendance\u0026quot;) + annotate(geom = \u0026quot;text\u0026quot;, x=83000, y= 55000, label = \u0026quot;Model under-predicted\\non these games\u0026quot;) # ggsave(\u0026quot;plots/prediction_plots/actual_v_pred.png\u0026quot;, width = 30, height = 22, units = \u0026quot;cm\u0026quot;) We can also look at how the model performed predicting crowds at each of the venues.\nafl_2019 %\u0026gt;% mutate(error = predicted_attendance - attendance) %\u0026gt;% ggplot(aes(x= error)) + geom_density() + geom_vline(xintercept = 0, linetype = 2) + facet_wrap(~ venue, scales = \u0026quot;free_y\u0026quot;, ncol = 2) + scale_x_continuous(labels = comma, name = \u0026quot;Error\u0026quot;) + ggtitle(\u0026quot;Exploring Errors by Venue\u0026quot;) + theme(axis.text.y = element_blank(), axis.title.y = element_blank()) # ggsave(\u0026quot;plots/prediction_plots/venue_errors.png\u0026quot;, width = 30, height = 22, units = \u0026quot;cm\u0026quot;) The model tended to over-predict attendances at the Gabba, Perth Stadium, Sydney Showgrounds and even the MCG, while it under-predicted at Carrara and “Other”.\nprint(paste0(\u0026quot;RMSE: \u0026quot;, sqrt(mean(error^2, na.rm = T)))) ## [1] \u0026quot;RMSE: 6601.69023180541\u0026quot; print(paste0(\u0026quot;MAE: \u0026quot;, mean(abs(error), na.rm = T))) ## [1] \u0026quot;MAE: 4881.5087837533\u0026quot;   Overall Performance of the Model The final model selected achieved an RMSE of 5,953 and MAE of 4,466, meaning that on average, the model is off by just under 4,500 fans when predicting attendances. When compared to the baseline model (RMSE of 6,816 and a MAE of 4,958), this model betters the baseline by over 500 people on average, but also doesn’t have as many extreme errors.\n Conclusion Hopefully you’ve made it to here!\nI’d love to hear from you with any feedback / corrections / etc in the comments or get in touch.\nThanks!\n ","date":1564185600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564185600,"objectID":"c796dcf16a607097e427af08e4d54565","permalink":"/blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/","publishdate":"2019-07-27T00:00:00Z","relpermalink":"/blog/building-a-linear-regression-model-in-r-to-predict-afl-crowds/","section":"post","summary":"Introduction There is a lot of talk about crowd behaviour and crowd issues with the modern day AFL. I personally feel the crowd issues are a case of business-as-usual; I’ve been going to AFL games on-and-off for close to three decades and the crowd behaviour is no different today as it was 20 years ago… just now everyone with a phone and Twitter account is a journo.","tags":[],"title":"Building a Linear Regression Model in R to Predict AFL Crowds","type":"post"},{"authors":null,"categories":["R","sport","afl","analytics","tidyverse"],"content":"  Introduction This post is the first in a two-part series on AFL crowds. This analysis will be a thorough look at AFL crowd numbers over the last 20 years, and will aim to discover which team’s fans are the biggest bandwagon jumpers. Bandwagon Jumpers are those fans that are the loudest in the office or in your friendship group when things are going well, and the quietest when their team is losing. The analysis will use two questions to answer this:\nWhich team’s home attendance is affected more because of the previous week’s performance Here I will introduce three new metrics to measure this effect  Does a team’s betting odds affect home attendance  People all have their theories on which team’s fans are the worst offending bandwagoners; this analysis will try to answer it once and for all! Also, sefishly, I’m sick of people telling me that fans of my team - the Hawthorn Hawks (a team that has experienced tremendous success over the last 10 years) are the worst offenders.\nThe Data Data was collected from a few different online sources. The game and attendance data was sourced using the rvest package from the amazing AFL Tables, while the betting data was sourced from the Australian Sports Betting website.\nAll the code and data for this project is available on GitHub. The Tidyverse suite of packages features heavily throughout.\nThe data is current to round 16 of the 2019 season.\nOver the last 20 years, the median (the 50% mark and a better measure than the average on skewed data) crowd to AFL games was 33,600. The data is positively skewed with a tail that extends out to 100,000.\nAs expected for the AFL, the finals series at the end of the season draws larger crowds than the premiership (regular season for non-AFL fans) season. The median attendance for the premiership season is just over 33,000, while for finals it is over 58,000. The finals series are the flagship games for the AFL - they would hope these games would draw larger crowds.\nWe can’t glean a lot from finals since they’re well patronised irrespective of who’s playing, so the rest of the analysis will focus on the premiership season.\nThe last 20 seasons have seen some peaks and throughs though, with median attendances ranging from just under 29,000 in 2012 to just over 36,800 in 2008. The low attendances in 2012-13 were no doubt fuelled by the introduction of the newest expansion teams - Gold Coast and Greater Western Sydney (GWS) - in regions that are traditionally not AFL strongholds. The last five years however have seen a rebound of sorts, as these clubs slowly build up fanbases (and in the case of GWS start to have some success).\n  Which teams draw larger attendances So what about the age old question asked around the water cooler… who has more fans?\nWell this post won’t be able to answer that question. What it can answer is which teams’s fans attend games in greater numbers?\nThere are seven teams that have had above average crowds since the 2000 season:\n Collingwood Essendon Richmond Adelaide Carlton West Coast Hawthorn  Four of those clubs are Victorian clubs, with the other two from South Australia and Western Australia. This is important to note that even though the AFL is a national competition, these three states are traditionally stronghold states. Brisbane - despite their extreme success in the early part of this century and the fact that a Victorian club merged with then (Fitzroy Lions) - have the third lowest attendances. Only recent expansion teams GWS and Gold Coast have lower attendances. The north-east states are clearly a problem for the league.\n Which team’s home attendance is affected more because of the previous week’s performance To answer this question, we can look at the difference in attendance for each team the next game after a win or a loss. Three metrics have been created to help answer this question:\n Losing Impact on Attendance (LIoA) Winning Impact on Attendance (WIoA) Overall Performance Impact (OPI)  To create the LIoA and WIoA metrics, the median home attendance (regadless if the previous game was a a win or loss) was calculated for each team. To calculate the LIoA, the median home attendance after a loss was claculated and the percentage difference between this and the overall median home attendance became the measure. The WIoA was calculated in almost the same way, but the median home attandance after a win was used instead.\nFinally, OPI is calculated as the difference between WIoA and LIoA.\nA few things to note using this method:\nRound one games have been excluded as the previous game was the last season and there’s too much noise using that Only home games for each team have been used to calculate the metrics With a p-value \u0026lt; 0.05, the team’s last result is a significant predictor  First thing we can do is visualise the data. Using boxplots to show the team’s distribution of home attendances for games both after a win and loss, we can see that there are what appear to be considerable differences for some teams. Where the line in the middle of the box (the median) differs noticably between a win or loss the previous week is an indication that that team’s supporters may be less inclined to attend their team’s games after a loss (or more inclined after a win).\nLosing Impact on Attendance (LIoA) Hmmmm… well that certainly doesn’t help my case. Hawthorn’s Losing Impact on Attendance (LIoA) rating is -11.9% - a league worst, meaning that Hawthorn fan attendance at games after a loss is 13% lower than their season median home attendance. Another Victorian club, St Kilda, aren’t far behind, with a LIoA of -10.7%. GWS and Collingwood follow closely, with an LIoA of -7.9 and -7.6% respectively.\nAt the other end of the spectrum, Geelong supporters appear to be unaffected by their team losing their prior game, with a 0.6% difference between their median home attendance. The Gold Coast Suns and South Australian club Adelaide are also fairly unaffected, with LIoA of -1.8% and -1.9% respectively.\n Winning Impact on Attendance (WIoA) Conversely, we can apply the same methodology to calculate a team’s Winning Impact on Attendance (WIoA) to determine which fans respond more positively after a win. The Melbourne Football Club’s fans appear to respond the most positively after their team win, with a WIoA of 13.6%. St Kilda are second on this list too, with a WIoA of 11.3% and North Melbourn are at 11.2%. Thankfully, my Hawks don’t top this measure, however with a WIoA of 10.3%, they’re not far behind.\nAgain, Geelong fans have the best WIoA at less than half a percent, while Gold Coast and Essendon are also both under 2%.\n Overall Performance Impact Calculating the difference between the two measures give an overall indicator of fan senstitvity to their team’s last performance. I hate to say it, but Hawthorn fans top this list with an OPI of 22.2%, just edging out St Kilda on 22.0%. Melbourne and GWS are at around the 18% mark, while North, Richmond, Collingwood and Carlton in the 16-17% range.\nThis pains me to say, but Geelong fans are the most consistent set of fans with an OPI of 1.1% Gold Coast, Adelaide and Essendon supporters are also fairly consistent in their attendance.\n  Team  Median Home Attendance  Attendance After Loss  Attendance After Win  LIoA  WIoA  OPI      Adelaide  41897.5  41095.0  42415.0  -0.0192  0.0124  0.0316    Brisbane Lions  25403.0  23943.0  26872.0  -0.0575  0.0578  0.1153    Carlton  35147.5  33115.5  38743.5  -0.0578  0.1023  0.1601    Collingwood  48261.0  44607.0  52592.0  -0.0757  0.0897  0.1654    Essendon  43947.0  42617.0  44736.0  -0.0303  0.018  0.0483    Fremantle  34553.0  33539.5  36026.0  -0.0293  0.0426  0.0719    Geelong  24857.0  24659.0  25078.0  -0.008  0.0089  0.0169    Gold Coast  13247.5  13080.0  13528.0  -0.0126  0.0212  0.0338    Greater Western Sydney  10069.5  9395.5  11071.5  -0.0669  0.0995  0.1664    Hawthorn  31925.0  28152.5  35202.0  -0.1182  0.1026  0.2208    Melbourne  28707.0  27266.0  32621.0  -0.0502  0.1363  0.1865    North Melbourne  24062.0  22754.0  26763.0  -0.0544  0.1123  0.1667    Port Adelaide  28206.0  26652.0  30197.0  -0.0551  0.0706  0.1257    Richmond  39713.5  36821.0  43240.0  -0.0728  0.0888  0.1616    St Kilda  30497.0  26974.0  33944.0  -0.1155  0.113  0.2285    Sydney  29264.0  27715.5  30863.5  -0.0529  0.0547  0.1076    West Coast  38029.0  36710.0  39436.0  -0.0347  0.037  0.0717    Western Bulldogs  27818.5  26301.0  28769.0  -0.0546  0.0342  0.0888       Does a team’s betting odds affect home attendance? To answer this question, betting data was collected from the Australian Sports Betting website.\nLine data was available from 2013 onwards, so the crowd data for this question has been limited down to the last six seasons.\nThe opening game lines were used for the Home Team. Where the line was within +/- 6 points (1 goal), the game was deemed to be Neutral, meaning no one was the favourite. If the line was set above 6 points, then the Home Team was the Favourite. Lastly, if the line was -6 points, the Home Team was the Underdog. This has also been found to be a significant predictor, also with a p-value close to zero.\nUnderdog Impact When the home team is the underdog, what is the impact to their home attendance?\nCarlton fans appear to be the most affected by their team’s chance of victory - when they’re the underdog, Carlton’s home attendance is 8.5% lower than their overall median home attendance. Strangely, Melbourne’s home attendance actually increases when they’re the underdog, with the attendance 13.7% higher.\nHawthorn feature near the top of this list too… not a good sign.\n Favourite Impact Wow… Ok this doesn’t look great. Hawthorn’s median attendance when favourite is over 45% higher than their overall home attendance since 2013. Geelong’s, just over 40%, is also abnormally high.\nWell, that didn’t work out how I expected. Rather than busting the myth that Hawthorn fans are the biggest bandwagon jumpers in the league, I’ve actually added to the argument.\nStay tuned for part two of this series, where I will attempt to build a predictive model that predicts AFL attendances.\nFeel free to leave any feedback / corrections / etc in the comments or get in touch.\n  ","date":1563148800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563148800,"objectID":"6253d8ae558556895e95c8d99eab957c","permalink":"/blog/using-data-to-determine-which-afl-fans-jump-onthe-bandwagon/","publishdate":"2019-07-15T00:00:00Z","relpermalink":"/blog/using-data-to-determine-which-afl-fans-jump-onthe-bandwagon/","section":"post","summary":"Introduction This post is the first in a two-part series on AFL crowds. This analysis will be a thorough look at AFL crowd numbers over the last 20 years, and will aim to discover which team’s fans are the biggest bandwagon jumpers.","tags":[],"title":"Using Data To Determine Which AFL Fans Jump On The Bandwagon","type":"post"}]